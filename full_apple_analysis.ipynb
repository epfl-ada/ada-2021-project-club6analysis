{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbed9bc1",
   "metadata": {},
   "source": [
    "# Import section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08478918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python standard library\n",
    "import json\n",
    "import datetime as dt\n",
    "\n",
    "# data science tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "\n",
    "\n",
    "# plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as plticker\n",
    "import seaborn as sns\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.stats import pearsonr, ttest_ind, mannwhitneyu\n",
    "\n",
    "# natural language processing tools\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# other helper tools\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b94c2e7",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "# SECTION 1: Initial analysis of Apple quotes\n",
    "When searching for Apple quotes, we must discard food-related quotes while including quotes about Apple Inc. Therefore, we will use a rule-based approach, where we have a list of indicator words that decide whether to include a quotation. This way, we will be left with a filtered dataset containing mostly quotations related to the company Apple. This also enables us to utilize more extensive filtration algorithms, which would be too costly to run on the entire dataset. We have created a data preprocessing pipeline, which as a result has created a csv-file for each year, consisting only of Apple-related quotes.\n",
    "\n",
    "Let's first load this data, as well as the relevant dates of the Apple product launches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6639e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial_analysis = pd.read_csv(\"data/quotes-2020-apple-filter.csv\",sep=\";\")\n",
    "df_initial_analysis = df_initial_analysis.append(pd.read_csv(\"data/quotes-2019-apple-filter.csv\",sep=\";\"))\n",
    "df_initial_analysis = df_initial_analysis.append(pd.read_csv(\"data/quotes-2018-apple-filter.csv\",sep=\";\"))\n",
    "df_initial_analysis = df_initial_analysis.append(pd.read_csv(\"data/quotes-2017-apple-filter.csv\",sep=\";\"))\n",
    "df_initial_analysis = df_initial_analysis.append(pd.read_csv(\"data/quotes-2016-apple-filter.csv\",sep=\";\"))\n",
    "df_initial_analysis = df_initial_analysis.append(pd.read_csv(\"data/quotes-2015-apple-filter.csv\",sep=\";\"))\n",
    "#List of dates for the apple events\n",
    "apple_event_dates_str=[\"2015-03-09\",\"2015-06-10\",\"2015-09-09\",\n",
    "\"2016-03-21\",\"2016-06-15\",\"2016-09-07\", \"2016-10-27\",\n",
    "\"2017-06-07\", \"2017-09-12\",\n",
    "\"2018-03-27\",\"2018-06-06\", \"2018-09-12\", \"2018-10-30\",\n",
    "\"2019-03-25\",\"2019-06-05\",\"2019-09-10\",\"2019-12-02\",\n",
    "\"2020-06-24\",\"2020-09-15\",\"2020-10-13\",\"2020-11-10\"]\n",
    "\n",
    "#apple_event_dates_pd = pd.DataFrame({'Date':[dt.datetime.strptime(date, \"%Y-%m-%d\").date() for date in apple_event_dates_str]})\n",
    "apple_event_dates=[dt.datetime.strptime(date, \"%Y-%m-%d\").date() for date in apple_event_dates_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eafd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bb162d",
   "metadata": {},
   "source": [
    "As seen above, we have gathered approximately 90.000 quotes from the quotebank data through 2015 to 2020. Lets start by investigating how the quote occurrences vary throughout the year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358ac41",
   "metadata": {},
   "source": [
    "## How does the daily number of quotes vary throughout the year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500b5249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the date column, such that it only contains date information and not timestamp\n",
    "df_initial_analysis['date_clean'] = df_initial_analysis.apply(lambda x: x['date'][:10],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2979da23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot to see the Apple-related quotes through the years\n",
    "time = df_initial_analysis.groupby(['date_clean']).sum().index\n",
    "quote_num = df_initial_analysis.groupby(['date_clean']).sum()['numOccurrences']\n",
    "fig,ax = plt.subplots(figsize=(18,8))\n",
    "ax.bar(time,quote_num,width=3, align='center')\n",
    "ax.set_xlabel(\"dates\",size=15)\n",
    "plt.xticks(time[::30],rotation=90)\n",
    "ax.set_ylabel(\"Quotation occurrences\",size=15)\n",
    "ax.set_title(\"Number of daily Apple-related Citations 2015 through 2020\",size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff593654",
   "metadata": {},
   "source": [
    "We see a massive spike close to the date 2018-04-23, which seems rather suspicious. Let's investigate this further, by finding the particular quote and print its information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269a381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print info about outlier quote\n",
    "print(f\"Quote occurrences: {df_initial_analysis.loc[df_initial_analysis.numOccurrences == df_initial_analysis.numOccurrences.max()].numOccurrences.item()} \")\n",
    "print(f\"Quote: {df_initial_analysis.loc[df_initial_analysis.numOccurrences == df_initial_analysis.numOccurrences.max()].quotation.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b8c966",
   "metadata": {},
   "source": [
    "It seems unlikely that such a generic quote has such a high occurrence number. There is the possibility that the quote originates from a viral tweet or advertisement, however, googling the quote yields no results. We will thus regard this as an extreme outlier for now, and investigate it further by comparing to the original data set in Milestone 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c700e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop outlier\n",
    "df_initial_analysis = df_initial_analysis.drop(df_initial_analysis.loc[df_initial_analysis.numOccurrences == 39978].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd924622",
   "metadata": {},
   "source": [
    "Now that the outlier has been dropped, let's take a look at the plot again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186dc50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot to see the Apple-related quotes through the years (without outlier)\n",
    "time = df_initial_analysis.groupby(['date_clean']).sum().index\n",
    "quote_num = df_initial_analysis.groupby(['date_clean']).sum()['numOccurrences']\n",
    "fig,ax = plt.subplots(figsize=(18,8))\n",
    "ax.bar(time,quote_num,width=3, align='center')\n",
    "ax.set_xlabel(\"dates\",size=15)\n",
    "plt.xticks(time[::30],rotation=90)\n",
    "ax.set_ylabel(\"Quotation occurrences\",size=15)\n",
    "ax.set_title(\"Number of daily Apple-related Citations 2015 through 2020\",size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e868b45",
   "metadata": {},
   "source": [
    "This looks more reasonable. Let's try to look if any of the peaks occur at the same time as apple events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f9b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add apple product launches as points\n",
    "fig,ax = plt.subplots(figsize=(18,8))\n",
    "ax.plot(time,quote_num, label= \"Quote occurrences\")\n",
    "ax.set_xlabel(\"dates\",size=15)\n",
    "ax.scatter(time[time.isin(apple_event_dates_str)],quote_num[time.isin(apple_event_dates_str)], c=\"r\", s=100, zorder=3, label='Apple events')\n",
    "plt.xticks(time[::30],rotation=90)\n",
    "ax.set_ylabel(\"Quotation occurrences\",size=15)\n",
    "ax.set_title(\"Number of daily Apple-related Citations 2015 through 2020\",size=20)\n",
    "ax.legend(prop={'size': 20})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97f62b5",
   "metadata": {},
   "source": [
    "Many of the peaks are very close to Apple product launches. In Milestone 3 we will investigate this further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94844243",
   "metadata": {},
   "source": [
    "## 3. How does the distribution of quote occurences look?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72fc2c0",
   "metadata": {},
   "source": [
    "We will now take a quick glance at how the distribution of Apple quotes look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2875bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(15,8))\n",
    "ax.hist(df_initial_analysis.numOccurrences[df_initial_analysis.numOccurrences < 60],bins=40,);\n",
    "ax.set_xlabel(\"Occurrence number\",size=15)\n",
    "ax.set_ylabel(\"Frequency\",size=15)\n",
    "ax.set_title(\"Distribution of quote occurrences (Occurrences capped at 60)\",size=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ff3910",
   "metadata": {},
   "source": [
    "As seen above, most of the quotes have been cited less than 10 times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31296888",
   "metadata": {},
   "source": [
    "## 4. How does the most frequent speakers vary throughout the year?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc219fd",
   "metadata": {},
   "source": [
    "### Investigation of frequent speakers\n",
    "\n",
    "We will now investigate which speakers are the most cited when it comes to Apple. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604a7282",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_speakers = df_initial_analysis.loc[df_initial_analysis.speaker != 'None'].groupby(by=['speaker']).sum().sort_values(by='numOccurrences',ascending=False).reset_index()\n",
    "fig,ax = plt.subplots(figsize=(20,8))\n",
    "ax.bar(top_speakers.speaker[:10],top_speakers.numOccurrences[:10])\n",
    "ax.set_xlabel(\"Author of quote\",size=15)\n",
    "ax.set_ylabel(\"Quotation occurrences\",size=15)\n",
    "#plt.xticks(top_speakers.speaker1[:10],rotation=90)\n",
    "ax.set_title(\"Authors with most qoute occurrences (2015-2020)\",size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3a6ddd",
   "metadata": {},
   "source": [
    "We see that Apple-related staff has the most quotes related to Apple e.g. Tim Cook, Steve Jobs, Eddy Cue, Jony Ive and Steve Wozniak. We now want to portray how this has evolved throughout the years. We will do this by making a column for each speaker. This enables us to plot the most active speakers over the period. Note that Donald Trump has several entries. The the code below will fix this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8a3c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting dataset and formatting date\n",
    "quotes_per_date = df_initial_analysis.copy()\n",
    "quotes_per_date['date'] = quotes_per_date['date'].apply(lambda x: dt.datetime.strptime(x[:10], '%Y-%m-%d'))\n",
    "\n",
    "#Extracting dataset and formatting date to show only month\n",
    "quotes_per_month = df_initial_analysis.copy()\n",
    "quotes_per_month['date'] = quotes_per_month['date'].apply(lambda x: dt.datetime.strptime(x[:7], '%Y-%m'))\n",
    "\n",
    "#Pivoting table such that each column is a speaker and index is date\n",
    "#Merging Donald Trump quotations and dropping None as well\n",
    "quotes_per_date = quotes_per_date.pivot_table('numOccurrences', ['date'], 'speaker', fill_value=0, aggfunc=np.sum)\n",
    "quotes_per_date['Donald Trump'] = quotes_per_date['Donald Trump'] + \\\n",
    "                                  quotes_per_date['President Donald Trump'] + \\\n",
    "                                  quotes_per_date['President Trump']\n",
    "quotes_per_date.drop(['None', 'President Donald Trump', 'President Trump'], axis=1, inplace=True)\n",
    "\n",
    "quotes_per_month = quotes_per_month.pivot_table('numOccurrences', ['date'], 'speaker', fill_value=0, aggfunc=np.sum)\n",
    "quotes_per_month['Donald Trump'] = quotes_per_month['Donald Trump'] + \\\n",
    "                                   quotes_per_month['President Donald Trump'] + \\\n",
    "                                   quotes_per_month['President Trump']\n",
    "quotes_per_month.drop(['None', 'President Donald Trump', 'President Trump'], axis=1, inplace=True)\n",
    "\n",
    "#Cummulative representation of quotes\n",
    "cummulative_quotes_per_date = quotes_per_date.cumsum(axis=0)\n",
    "cummulative_quotes_per_month = quotes_per_month.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6986c7f7",
   "metadata": {},
   "source": [
    "Due to visibility reasons we have chosen to only display the three most active speakers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c94c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the 3 most frequent speakers\n",
    "top_quoters = quotes_per_month.sum(axis=0).nlargest(3).index\n",
    "\n",
    "#Extracting these from week dataset\n",
    "top_quotes_per_month = quotes_per_month[top_quoters]\n",
    "cummulative_top_quotes_per_month = cummulative_quotes_per_month[top_quoters]\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "labels=[]\n",
    "for quoter in top_quoters:\n",
    "    plt.plot(top_quotes_per_month.index, top_quotes_per_month[quoter])\n",
    "    labels.append(quoter)\n",
    "\n",
    "plt.legend(labels, ncol=2, loc='upper left', fancybox=True, shadow=True,\n",
    "           fontsize=12)\n",
    "fig.suptitle('Quotation occurences per month (3 most quoted speakers)', fontsize=30)\n",
    "plt.xlabel('Year', fontsize=15)\n",
    "plt.ylabel('Number of quotations', fontsize=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79460608",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_quoters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbed0569",
   "metadata": {},
   "source": [
    "As seen above, each speaker has certain periods where they are quoted a lot, causing several spikes in the graph. To smooth this out we will plot the cumulative distribution over the same period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6578962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the 6 most frequent speakers\n",
    "top_quoters = quotes_per_month.sum(axis=0).nlargest(6).index\n",
    "\n",
    "#Extracting these from week dataset\n",
    "top_quotes_per_month = quotes_per_month[top_quoters]\n",
    "cummulative_top_quotes_per_month = cummulative_quotes_per_month[top_quoters]\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "labels=[]\n",
    "for quoter in top_quoters:\n",
    "    plt.plot(cummulative_top_quotes_per_month.index, cummulative_top_quotes_per_month[quoter])\n",
    "    labels.append(quoter)\n",
    "\n",
    "plt.legend(labels, ncol=2, loc='upper left', fancybox=True, shadow=True,\n",
    "           fontsize=12)\n",
    "fig.suptitle('Cummulative quotation occurences per week', fontsize=30)\n",
    "plt.xlabel('Week number', fontsize=15)\n",
    "plt.ylabel('Number of quotations', fontsize=15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4817ca",
   "metadata": {},
   "source": [
    "We immediately recognize the big names *Donald Trump*, *Tim Cook* and *Steve Jobs*. Although Steve Jobs has passed away, he frequently gets quoted in the media. In addition, *Jony Ive* is the former chief designer of Apple, whom left the company in 2019. *Eddy Cue* is the senior vice president of Apple, reporting to Tim Cook. So it is natural that these names appear. *Phil Schiller* is also an apple executive. We initially observe that the most frequent speakers of Apple are indeed a part of the company. Not very shocking.\n",
    "\n",
    "The problem with the plot above is that it only shows the speakers with the most attributed quotes throughout the whole period. To get a more dynamic view over the whole period, we've made an animation which shows the most frequent speakers up to each point in time. This can be viewed in the README."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c19e40",
   "metadata": {},
   "source": [
    "### Code for making a .mp4 file with the animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3781fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(values, ranks):\n",
    "    total_quotes = int(round(values.sum(), -2))\n",
    "    s = f'Total Quotes - {total_quotes:,.0f}'\n",
    "    return {'x': .99, 'y': .05, 's': s, 'ha': 'right', 'size': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bf2606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine whether to make mp4 visualization of most quoted speakers. NOTE: To run this code 'bar_chart_race' and 'ffmpeg' needs to be installed in the environment.\n",
    "create_mp4 = False\n",
    "if create_mp4:\n",
    "    quotes_per_date_plot = bcr.bar_chart_race(cummulative_quotes_per_month, filename='quotes_per_date_plot.mp4',\n",
    "                                          n_bars=10,\n",
    "                                          filter_column_colors=True,\n",
    "                                          period_length=1000,\n",
    "                                          steps_per_period=20,\n",
    "                                          bar_label_size=7,\n",
    "                                          tick_label_size=7,\n",
    "                                          title='Most frequently quoted speakers about Apple',\n",
    "                                          period_fmt='%B %Y',  \n",
    "                                          dpi=500,\n",
    "                                          shared_fontdict={'family' : 'serif', 'color' : 'black', 'size':'8', 'weight': 'normal'},\n",
    "                                          cmap='dark12',\n",
    "                                          period_summary_func=summary)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b769c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Initial analysis of Apple stock and events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4180599e",
   "metadata": {},
   "source": [
    "Since a huge part of this project will be to compare the quote data with stock data, we will now look at the Apple stock data, events and quarterly earnings for Apple Inc. \n",
    "We start by loading in the data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f3d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load stock data\n",
    "stock_data_initial=pd.read_csv('./data/AAPL_2015_to_2020_yahoo_finance.csv')\n",
    "stock_data_initial.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7040104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load quarterly earnings data\n",
    "quarterly_earnings=pd.read_excel('./data/Earnings_Apple.xlsx')\n",
    "quarterly_earnings.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f8809f",
   "metadata": {},
   "source": [
    "We see that the first 8 rows of the `quarterly_earnings` data frame are irrelevant for this study. We, therefore, remove those.\n",
    "We also observe that the `quarterly_earnings` and `stock_data_initial` have a different format of their Date column. We need to change one of them to be able to compare the two columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a44d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the unnecessary rows in the quarterly_earnings\n",
    "quarterly_earnings=quarterly_earnings.drop(index=[0,1,2,3,4,5,6,7])\n",
    "\n",
    "#Change the format of the Earnings Date column to be able to compare it with the Stock dataframe\n",
    "quarterly_earnings[\"Earnings Date\"]=quarterly_earnings[\"Earnings Date\"].map(lambda x: x.replace(\", 12 AMEST\", \"\").replace(\",\", \"\"))\n",
    "quarterly_earnings[\"Earnings Date\"]=quarterly_earnings[\"Earnings Date\"].map(lambda x: dt.datetime.strptime(x, \"%b %d %Y\").strftime(\"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c087c200",
   "metadata": {},
   "source": [
    "We now create a new dataframe containing only stock data from earning dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61469dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare Dates in the two dataframe, based on this we create a new dataframe\n",
    "apple_earnings_and_stock_data=stock_data_initial[stock_data_initial.Date.isin(quarterly_earnings[\"Earnings Date\"])]\n",
    "apple_earnings_and_stock_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d91bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new dataframe only containing stock data from Apple event-days\n",
    "apple_event_and_stock_data=stock_data_initial[stock_data_initial.Date.isin(apple_event_dates_str)]\n",
    "apple_event_and_stock_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb465b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new dataframe only containing stock data from Apple event-days\n",
    "apple_event_and_stock_data=stock_data_initial[stock_data_initial.Date.isin(apple_event_dates_str)]\n",
    "apple_earnings_and_stock_data=stock_data_initial[stock_data_initial.Date.isin(quarterly_earnings[\"Earnings Date\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7557ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apple_event_and_stock_data.to_csv('./data/events_dates_with_stock_data.csv')\n",
    "#apple_earnings_and_stock_data.to_csv('./data/earnings_dates_with_stock_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a39f2b",
   "metadata": {},
   "source": [
    "## Add data to dates where the stock market is closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40483b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data_initial.head(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8654484b",
   "metadata": {},
   "source": [
    "We see that the `stock_data_initial` don't contain all dates due to bank holidays and weekends. This may create conflicts when we want to merge the quotes dataset with the `stock_data_initial`. We will therefore look into different solutions in solving this problem.\n",
    "First we try to fill the left out dates with the last valid observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe7483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and fill missing dates\n",
    "stock_data_with_closed_days=stock_data_initial.copy()\n",
    "idx = pd.date_range('2015-01-02', '2020-12-31')\n",
    "stock_data_with_closed_days.index = pd.DatetimeIndex(stock_data_initial.Date)\n",
    "stock_data_with_closed_days = stock_data_with_closed_days.reindex(idx, method='ffill').reset_index()\n",
    "stock_data_with_closed_days = stock_data_with_closed_days.drop(columns=['Date'])\n",
    "stock_data_with_closed_days = stock_data_with_closed_days.rename(columns={\"index\": \"Date\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7655c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data_with_closed_days.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e31e7a5",
   "metadata": {},
   "source": [
    "We now see that all dates has observations in it.\n",
    "\n",
    "We can then plot the event dates, quarterly earning dates and stock price to see if there is any correlation between the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3defdf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots the dates after adding the \n",
    "fig,ax = plt.subplots(figsize=(18,8))\n",
    "ax.plot(stock_data_with_closed_days.Date,stock_data_with_closed_days.Close, label='Apple stock')\n",
    "ax.set_xlabel(\"Date (daily)\", size=18)\n",
    "ax.set_ylabel('Stock closing price ($)', size=18)\n",
    "plt.xticks(stock_data_with_closed_days.Date[::366])\n",
    "ax.scatter(apple_event_and_stock_data.Date, apple_event_and_stock_data.Close, c=\"r\", s=100, zorder=3, label='Apple events')\n",
    "ax.scatter(apple_earnings_and_stock_data.Date, apple_earnings_and_stock_data.Close, c=\"g\", s=100, zorder=3, label='Apple earning dates')\n",
    "ax.legend(prop={'size': 25})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2f45ae",
   "metadata": {},
   "source": [
    "In addition we plot a graph with the trading volume for each day, to see if there are higher trading volumes around the event and quarterly earning dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5ca995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot each trading day and its volume\n",
    "fig,ax = plt.subplots(figsize=(18,8))\n",
    "ax.bar(stock_data_with_closed_days.Date,stock_data_with_closed_days.Volume, label='Apple stock', width=3, align='center')\n",
    "ax.set_xlabel(\"Date (daily)\",size=15)\n",
    "ax.set_ylabel('Daily trading volume', size=18)\n",
    "plt.xticks(stock_data_with_closed_days.Date[::366])\n",
    "ax.scatter(apple_event_and_stock_data.Date, apple_event_and_stock_data.Volume, c=\"r\", s=100, zorder=3, label='Apple events')\n",
    "ax.scatter(apple_earnings_and_stock_data.Date, apple_earnings_and_stock_data.Volume, c=\"g\", s=100, zorder=3, label='Apple earning dates')\n",
    "ax.legend(prop={'size': 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3781a0f4",
   "metadata": {},
   "source": [
    "Interestingly, it seems that some events occur in periods with high trading volume. In milestone 3 we want to investigate how this correlates to the media attention. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea73653",
   "metadata": {},
   "source": [
    "## Group by week"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae9e7b7",
   "metadata": {},
   "source": [
    "Quote values may fluctuate heavily from day to day, and it is thus reasonable to aggregate in weeks. When comparing quote values to stocks, we thus need to be able to aggregate stock data on a weekly basis. Instead of imputing data into the `stock_data_initial`, we thus try to average the values per week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb032c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data_weekly_average=stock_data_initial.copy()\n",
    "\n",
    "#Create a new dataframe containing the weekly average of the stock price. The method is obtained from\n",
    "stock_data_weekly_average.Date = pd.to_datetime(stock_data_weekly_average.Date) - pd.to_timedelta(7, unit='d')\n",
    "stock_data_weekly_average = stock_data_weekly_average.groupby([pd.Grouper(key='Date', freq='W-MON')]).mean().reset_index().sort_values('Date')\n",
    "stock_data_weekly_average.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5c4ae2",
   "metadata": {},
   "source": [
    "We then plot the new graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291eddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots the weekly average\n",
    "fig,ax = plt.subplots(figsize=(18,8))\n",
    "ax.plot(stock_data_weekly_average.Date,stock_data_weekly_average.Close, label='Apple stock')\n",
    "apple_events=ax.set_xlabel(\"Date (weekly)\",size=18)\n",
    "ax.set_ylabel('Stock closing price ($)', size=18)\n",
    "ax.scatter(apple_event_and_stock_data.Date, apple_event_and_stock_data.Close, c=\"r\", s=100, zorder=3, label='Apple events')\n",
    "ax.scatter(apple_earnings_and_stock_data.Date, apple_earnings_and_stock_data.Close, c=\"g\", s=100, zorder=3, label='Apple earnings date')\n",
    "ax.legend(prop={'size': 25})\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f523dc",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br>\n",
    "# SECTION 2: Sentiment analysis of Apple quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000f362e",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Creating the sentiment dataframe\n",
    "First we are extracting the apple data from the already filtered csv-files. These were created using the Extraction_of_relevant_apple_data.ipynb jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f75a05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment_analysis = pd.read_csv(\"data/quotes-2020-apple-filter.csv\",sep=\";\")\n",
    "df_sentiment_analysis = df_sentiment_analysis.append(pd.read_csv(\"data/quotes-2019-apple-filter.csv\",sep=\";\"))\n",
    "df_sentiment_analysis = df_sentiment_analysis.append(pd.read_csv(\"data/quotes-2018-apple-filter.csv\",sep=\";\"))\n",
    "df_sentiment_analysis = df_sentiment_analysis.append(pd.read_csv(\"data/quotes-2017-apple-filter.csv\",sep=\";\"))\n",
    "df_sentiment_analysis = df_sentiment_analysis.append(pd.read_csv(\"data/quotes-2016-apple-filter.csv\",sep=\";\"))\n",
    "df_sentiment_analysis = df_sentiment_analysis.append(pd.read_csv(\"data/quotes-2015-apple-filter.csv\",sep=\";\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9348b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment_analysis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0082795",
   "metadata": {},
   "source": [
    "Changing the dataframe columns to their appropriate data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cb7fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment_analysis.quoteID = df_sentiment_analysis.quoteID.astype('string')\n",
    "df_sentiment_analysis.quotation = df_sentiment_analysis.quotation.astype('string')\n",
    "df_sentiment_analysis.speaker = df_sentiment_analysis.speaker.astype('string')\n",
    "df_sentiment_analysis.numOccurrences = df_sentiment_analysis.numOccurrences.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2e200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment_analysis.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582094ca",
   "metadata": {},
   "source": [
    "Sorting the dataframe by date so we get the correct order of the dataframes for the sentiment graph that will be created later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82defb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment_analysis = df_sentiment_analysis.sort_values('date')\n",
    "df_sentiment_analysis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67db4444",
   "metadata": {},
   "source": [
    "For each row in the dataframe we will attach 4 new columns, neg, neu, pos and compound. These values are the negative, neutral, positive and compound values we get from the SentimentIntensityAnalyzer polary scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba615ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell takes 20 minutes to run, don't run it if not necessary.\n",
    "# Run it ones and then read from csv\n",
    "\n",
    "column_names = ['quoteID', 'quotation', 'speaker', 'date', 'numOccurrences','neg', 'neu', 'pos', 'compound']\n",
    "    \n",
    "sentiment_info_df = pd.DataFrame(columns = column_names)\n",
    "\n",
    "print(\"Total number of rows: \", df_sentiment_analysis.shape[0])\n",
    "for idx, row in tqdm(df_sentiment_analysis.iterrows()):\n",
    "    \n",
    "    # Calculating scores from SentimentIntensityAnalyzer\n",
    "    new_row_dct = SentimentIntensityAnalyzer().polarity_scores(row['quotation'])\n",
    "    \n",
    "    # Creating and writing over value    \n",
    "    new_row_dct['quoteID'] = row['quoteID']\n",
    "    new_row_dct['quotation'] = row['quotation']\n",
    "    new_row_dct['speaker'] = row['speaker']\n",
    "    new_row_dct['date'] = row['date']\n",
    "    new_row_dct['numOccurrences'] = row['numOccurrences']\n",
    "    new_row = pd.DataFrame(new_row_dct, columns=column_names, index=[0])\n",
    "    sentiment_info_df = sentiment_info_df.append(new_row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0d660e",
   "metadata": {},
   "source": [
    "We will also attach a date_clean column to the dataframe which only contains the date and not the timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e274f31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting by datetime\n",
    "sentiment_info_df_sorted = sentiment_info_df.set_index('quoteID').sort_values('date')\n",
    "\n",
    "# Clean the date column, such that it only contains date information and not timestamp\n",
    "sentiment_info_df_sorted['date_clean'] = sentiment_info_df_sorted.apply(lambda x: x['date'][:10],axis=1)\n",
    "sentiment_info_df_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0eca74",
   "metadata": {},
   "source": [
    "We will save the current dataframe to csv since the analysis part takes around 20 minutes, so we don't have to redo that part each time we are going to change something in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f83c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment_info_df_sorted.to_csv('./data/sentiment_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c69db5",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Part 2: Sentiment analysis\n",
    "Here we firstly read the dataframe from the csv and remove the fortnite outlier quote, which contains 39 978 occurences. This quote was decided to be removed in the exploratory part of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44d86ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from csv to recreate a the fresh dataframe\n",
    "sentiment_df = pd.read_csv('./data/sentiment_df.csv')\n",
    "#finding index of outlier\n",
    "print(sentiment_df[sentiment_df.numOccurrences == 39978])\n",
    "#removing outlier\n",
    "sentiment_df = sentiment_df.drop(58674)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d4ae07",
   "metadata": {},
   "source": [
    "**Raw datapoints visualized:**  \n",
    "Firstly we will visualize the datapoints for positive and negative tweets.\n",
    "- Green points: positive tweets\n",
    "- Red points: negative tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dd665f",
   "metadata": {},
   "source": [
    "This visualization is mainly to get a feeling of how to points are distributed, but it won't give us a lot of information straight away. It is just too much information for a human being to interpret, we will therefore make an aggregated graph based on the positive and negative points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc43d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_sentiment_analysis,ax_sentiment_analysis = plt.subplots(figsize=(25,10))\n",
    "\n",
    "cutoff_decimal = 0.0\n",
    "\n",
    "\n",
    "pos_scores_df = sentiment_df[sentiment_df['compound'] > cutoff_decimal]\n",
    "pos_scores = pos_scores_df['compound']\n",
    "pos_dates = pos_scores_df['date_clean']\n",
    "\n",
    "neg_scores_df = sentiment_df[sentiment_df['compound'] < -cutoff_decimal]\n",
    "neg_scores = -neg_scores_df['compound']\n",
    "neg_dates = neg_scores_df['date_clean']\n",
    "ax_sentiment_analysis.set_ylim([0.0,1])\n",
    "ax_sentiment_analysis.scatter(pos_dates, pos_scores, s=5, color='green')\n",
    "ax_sentiment_analysis.scatter(neg_dates, neg_scores, s=5, color='red')\n",
    "\n",
    "loc = plticker.MultipleLocator(base=50) # this locator puts ticks at regular intervals\n",
    "ax_sentiment_analysis.xaxis.set_major_locator(loc)\n",
    "plt.title('Visualizing all sentiment intensity scores', fontsize=18)\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Sentiment intensity scores for each tweet in dataset', fontsize=17)\n",
    "ax_sentiment_analysis.tick_params(axis='both', which='major', labelsize=18)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69484034",
   "metadata": {},
   "source": [
    "## 1 month aggregated sentiment intensity scores\n",
    "\n",
    "Positive tweets will account for their value (given from SentimentIntensityAnalyzer) times +1 and times numOccurences.  \n",
    "Negative tweets will account for their value (given from SentimentIntensityAnalyzer) times -1 and times numOccurences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b673df",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_month_sentiment = []\n",
    "agg_month_dates = []\n",
    "\n",
    "for index, (idx, row) in enumerate(sentiment_df.iterrows()):\n",
    "    date = row['date_clean']\n",
    "    if index == 0:\n",
    "        current_month = date[:7]\n",
    "        current_size = 0\n",
    "\n",
    "\n",
    "    tmp_month = current_month\n",
    "    current_month = date[:7] # the seven first digits of the date\n",
    "    if tmp_month != current_month:\n",
    "        agg_month_sentiment.append(current_size)\n",
    "        current_size = 0\n",
    "        agg_month_dates.append(current_month)\n",
    "    \n",
    "    # updates weighted aggregated value of positive and negative\n",
    "    \n",
    "    current_size += row['compound']*row['numOccurrences']\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf41832e",
   "metadata": {},
   "source": [
    "**For the datastory:**\n",
    "For our sentiment analysis, we have labeled every quote in our apple dataset with a sentiment compound score between -1.0 and 1.0. Since there are too many quotes to represent visually, we have aggregated quote scores for each month and visualized the aggregated score for that month. These calculations heavily depend on the sentiment tool, so to secure that we don't include neutral quotes or quotes that the tool is hesitant about, we have made an arbitrarily cutoff of 0.6. Therefore, the graph will not include sentiment intensity scores between -0.6 and 0.6 in the calculations.\n",
    "\n",
    "Comments on the graph: As we can see by the chart there seems to be a disproportionate balance between positive and negative quotes since most months, the aggregated score is far above 0. We can also see a reduction in sentiment intensity scores from late 2019 until early 2020 - which may be caused by covid-19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6a2e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_1_sentiment, ax_1_sentiment = plt.subplots(figsize=(18,8))\n",
    "result_values = []\n",
    "result_mon = []\n",
    "for mon, agg in zip(agg_month_dates, agg_month_sentiment):\n",
    "    result_values.append(agg)\n",
    "    result_mon.append(mon)\n",
    "\n",
    "sns.set_theme(style=\"ticks\", rc={\"axes.spines.right\": False, \"axes.spines.top\": False},\n",
    "              font_scale=1.8, font=\"PT Sans\")\n",
    "\n",
    "\n",
    "ax_1_sentiment.plot(result_mon, result_values, '-o', color='#800020')\n",
    "#plt.title('1 month aggregation scores for tweets', fontsize=18)\n",
    "plt.xlabel('Date', fontsize=24)\n",
    "plt.ylabel('Aggregated sentiment intensity score', fontsize=24)\n",
    "\n",
    "#ax_1_sentiment.plot(agg_month_dates, agg_month_sentiment)\n",
    "plt.xticks(rotation=45);\n",
    "for n, label in enumerate(ax_1_sentiment.xaxis.get_ticklabels()):\n",
    "    if n % 12 != 0:\n",
    "        label.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26808290",
   "metadata": {},
   "source": [
    "## 1 month aggregated and weighted sentiment intensity scores (based on speaker)\n",
    "**Name list - multiplicative weight:**\n",
    "1. Tim Cook  - 2.0\n",
    "2. Steve Jobs - 1.9\n",
    "3. Eddy Cue - 1.8\n",
    "4. Jony Ive - 1.7\n",
    "5. Donald Trump - 1.6\n",
    "6. Phill Schiller - 1.5\n",
    "7. Jeff Williams - 1.4\n",
    "8. Steve Wozniak - 1.3\n",
    "9. Ben Wood  -  1.2\n",
    "10. Brian White  - 1.1\n",
    "\n",
    "Based on our \"Most frequently quoted speakers about Apple\" bar chart race, we will give quotes that has a speaker to any of the people from the top 10 list some additional weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3859745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_weights = {\n",
    "    \"Tim Cook\": 2,\n",
    "    \"Steve Jobs\": 1.9,\n",
    "     \"Eddy Cue\" : 1.8,\n",
    "     \"Jony Ive\" : 1.7,\n",
    "     \"Donald Trump\" : 1.6,\n",
    "     \"Phill Schiller\" : 1.5,\n",
    "     \"Jeff Williams\" : 1.4,\n",
    "     \"Steve Wozniak\" : 1.3,\n",
    "     \"Ben Wood\" : 1.2,\n",
    "     \"Brian White\" : 1.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed67962",
   "metadata": {},
   "source": [
    "In this code block we do the same as we did in the one above, the only difference is that we scale the datapoint if it is included in the speaker weights dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f1583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_month_speaker_weight_sentiment = []\n",
    "agg_month_speaker_weight_dates = []\n",
    "\n",
    "for index, (idx, row) in enumerate(sentiment_df.iterrows()):\n",
    "    date = row['date_clean']\n",
    "    speaker = row['speaker']\n",
    "    if index == 0:\n",
    "        current_speaker_weight_month = date[:7]\n",
    "        current_speaker_weight_size = 0\n",
    "\n",
    "\n",
    "    tmp_speaker_weight_month = current_speaker_weight_month\n",
    "    current_speaker_weight_month = date[:7] # the seven first digits of the date\n",
    "    if tmp_speaker_weight_month != current_speaker_weight_month:\n",
    "        agg_month_speaker_weight_sentiment.append(current_speaker_weight_size)\n",
    "        current_speaker_weight_size = 0\n",
    "        agg_month_speaker_weight_dates.append(current_speaker_weight_month)\n",
    "    \n",
    "    speaker_mult = 1\n",
    "    if speaker in speaker_weights:\n",
    "        speaker_mult = speaker_weights[speaker]\n",
    "        \n",
    "    # updates weighted aggregated value of positive and negative\n",
    "    if abs(row['compound']) > 0.6:\n",
    "        current_speaker_weight_size += row['compound']*row['numOccurrences']*speaker_mult\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41c3afb",
   "metadata": {},
   "source": [
    "Here we have the 1 month aggregations scores which are multiplied by the speaker weights, which gives extra weight to the important speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8a8275",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_2_sentiment, ax_2_sentiment = plt.subplots(figsize=(18,8))\n",
    "result_values = []\n",
    "result_mon = []\n",
    "for mon, agg in zip(agg_month_speaker_weight_dates, agg_month_speaker_weight_sentiment):\n",
    "    result_values.append(agg)\n",
    "    result_mon.append(mon)\n",
    "\n",
    "        \n",
    "\n",
    "ax_2_sentiment.scatter(result_mon, result_values)\n",
    "\n",
    "ax_2_sentiment.plot(result_mon, result_values, '-o', color='green')\n",
    "plt.title('1 month aggregation scores for tweets with additional speaker weights', fontsize=18)\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Aggregated sentiment intensity score', fontsize=18)\n",
    "\n",
    "\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ca8f98",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 1 week aggregated sentiment intensity scores\n",
    "In this graph we will aggregate on each week, i.e. every 7 day we will make a data point on the graph. It is calculated counting one for each time the days change, when i reaches seven we \"submit\" the aggregated scores to the agg_week_sentiment list, which we will use for plotting. This way we get the aggregated scores for a 7 day period i.e. a week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7337e85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_week_sentiment_weekly = []\n",
    "agg_week_dates_weekly = []\n",
    "day_counter_weekly = 0\n",
    "for index, (idx, row) in enumerate(sentiment_df.iterrows()):\n",
    "    date = row['date_clean']\n",
    "    if index == 0:\n",
    "        current_day_weekly = date[:10]\n",
    "        current_size_weekly = 0\n",
    "\n",
    "\n",
    "    tmp_day_weekly = current_day_weekly\n",
    "    current_day_weekly = date[:10] # the seven first digits of the date\n",
    "    if tmp_day_weekly != current_day_weekly:\n",
    "        day_counter_weekly += 1\n",
    "    if day_counter_weekly == 7:\n",
    "        agg_week_sentiment_weekly.append(current_size_weekly)\n",
    "        agg_week_dates_weekly.append(current_day_weekly)\n",
    "        current_size_weekly = 0\n",
    "        day_counter_weekly = 0\n",
    "\n",
    "    # updates weighted aggregated value of positive and negative\n",
    "    current_size_weekly += row['compound']*row['numOccurrences']\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab3cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_3_sentiment, ax_3_sentiment = plt.subplots(figsize=(18,8))\n",
    "result_week_values = []\n",
    "result_week = []\n",
    "for week, agg in zip(agg_week_dates_weekly, agg_week_sentiment_weekly):\n",
    "    result_week_values.append(agg)\n",
    "    result_week.append(week)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "ax_3_sentiment.plot(result_week, result_week_values, color='green')\n",
    "loc_weekly = plticker.MultipleLocator(base=5) # this locator puts ticks at regular intervals\n",
    "ax_3_sentiment.xaxis.set_major_locator(loc_weekly)\n",
    "plt.title('1 week aggregation scores for tweets with additional speaker weights', fontsize=18)\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Aggregated sentiment intensity score', fontsize=18)\n",
    "\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b08bb7d",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 1 day aggregated sentiment intensity scores\n",
    "In this graph we will aggregated quotes from each day, i.e. every unique day we will make a data point on the graph. It is calculated by \"submitting\" the scores every time the day changes in dateframe. This way we get the aggregated scores for each unique day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ca8aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33a5222",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_sentiment_daily = []\n",
    "agg_dates_daily = []\n",
    "\n",
    "for index, (idx, row) in enumerate(sentiment_df.iterrows()):\n",
    "    date = row['date_clean']\n",
    "    if index == 0:\n",
    "        current_day_daily = date[:10]\n",
    "        current_size_daily = 0\n",
    "\n",
    "\n",
    "    tmp_day_daily = current_day_daily\n",
    "    current_day_daily = date[:10] # the seven first digits of the date\n",
    "    if tmp_day_daily != current_day_daily:\n",
    "        agg_sentiment_daily.append(current_size_daily)\n",
    "        current_size_daily = 0\n",
    "        agg_dates_daily.append(current_day_daily)\n",
    "\n",
    "    # updates weighted aggregated value of positive and negative\n",
    "    current_size_daily += row['compound']*row['numOccurrences']\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4c251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_4_sentiment, ax_4_sentiment = plt.subplots(figsize=(18,8))\n",
    "result_daily_values = []\n",
    "result_daily = []\n",
    "for day, agg in zip(agg_dates_daily, agg_sentiment_daily):\n",
    "    result_daily_values.append(agg)\n",
    "    result_daily.append(day)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "ax_4_sentiment.plot(result_daily, result_daily_values, color='green')\n",
    "loc_daily = plticker.MultipleLocator(base=30) # this locator puts ticks at regular intervals\n",
    "ax_4_sentiment.xaxis.set_major_locator(loc_daily)\n",
    "plt.title('1 day aggregation scores for tweets with additional speaker weights', fontsize=18)\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Aggregated sentiment intensity score', fontsize=18)\n",
    "\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c346b9",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Part 3: Word cloud\n",
    "In this section we will look at the word clouds for a positive and negative quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e42bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_quotation_text = \"\"\n",
    "\n",
    "for index, (idx, row) in tqdm(enumerate(sentiment_df.iterrows())):\n",
    "\n",
    "    all_quotation_text += \" \" + row['quotation']\n",
    "\n",
    "        \n",
    "for string in ['Apple', 'apple', 'iPad', 'iPhone', 'Apple watch', 'apples', 'Apples', 'iPhones', ' S ', ' s ']:\n",
    "    all_quotation_text = all_quotation_text.replace(string, \"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438f1518",
   "metadata": {},
   "source": [
    "**Removing words that are products:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c233433",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(STOPWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db4af65",
   "metadata": {},
   "source": [
    "### Word cloud for positive quotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cf379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array(Image.open(\"./data/apple_logo.png\"))\n",
    "\n",
    "\n",
    "word_cloud = WordCloud(stopwords=stopwords, background_color=\"white\", mask=mask, mode=\"RGBA\", max_words=500)\n",
    "wc_all_quotes = word_cloud.generate(all_quotation_text)\n",
    "plt.figure()\n",
    "image_colors = ImageColorGenerator(mask)\n",
    "plt.figure(figsize=[20,20])\n",
    "plt.imshow(wc_all_quotes.recolor(color_func=image_colors), interpolation='bilinear')\n",
    "plt.axis(\"off\");\n",
    "plt.savefig(\"./data/apple_word_cloud.png\", format=\"png\") \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3f1671",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "# SECTION 3: Stock sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a23114",
   "metadata": {},
   "source": [
    "## Preprocessing of data from stock analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7736c4",
   "metadata": {},
   "source": [
    "#### Load stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data=pd.read_csv('./data/AAPL_2015_to_2020_yahoo_finance.csv')\n",
    "apple_event_and_stock_data=pd.read_csv('./data/events_dates_with_stock_data.csv')\n",
    "apple_earnings_and_stock_data=pd.read_csv('./data/earnings_dates_with_stock_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebc6c25",
   "metadata": {},
   "source": [
    "#### Remove unnecessary data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115b3ec6",
   "metadata": {},
   "source": [
    "Remove the stock data after 16-04-2020 due to the lack of sentiment data after that date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af9e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data=stock_data[~(stock_data['Date'] > '2020-04-16')]\n",
    "apple_event_and_stock_data=apple_event_and_stock_data[~(apple_event_and_stock_data['Date'] > '2020-04-16')]\n",
    "apple_earnings_and_stock_data=apple_earnings_and_stock_data[~(apple_earnings_and_stock_data['Date'] > '2020-04-16')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34840d67",
   "metadata": {},
   "source": [
    "#### Convert the Date values into datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d40b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data[\"Date\"]=stock_data['Date'].apply(lambda x: dt.datetime.strptime(x[:10], '%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ce630a",
   "metadata": {},
   "source": [
    "### Calculate weekly average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5677e28c",
   "metadata": {},
   "source": [
    "When plotting the stock price development, we want to use the weekly average price of the stock. This is due to the missing stock data for bank holidays and weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26913a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data_weekly_average=stock_data.copy()\n",
    "\n",
    "#Create a new dataframe containing the weekly average of the stock price.\n",
    "stock_data_weekly_average.Date = pd.to_datetime(stock_data_weekly_average.Date) - pd.to_timedelta(7, unit='d')\n",
    "stock_data_weekly_average = stock_data_weekly_average.groupby([pd.Grouper(key='Date', freq='W-MON')]).mean().reset_index().sort_values('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffca02b6",
   "metadata": {},
   "source": [
    "### Calculate monthly change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c1650b",
   "metadata": {},
   "source": [
    "Measurre the monthly change of the stock price. This will be used later when comparing the monthly price change and monthly sentiment score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4975c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_open_prize=stock_data.groupby(pd.Grouper(key='Date', freq='M')).first()['Open']\n",
    "monthly_close_price=stock_data.groupby(pd.Grouper(key='Date', freq='M')).last()['Close']\n",
    "\n",
    "stock_diff_monthly=pd.DataFrame(((monthly_close_price-monthly_open_prize)/monthly_open_prize)*100, columns=['Price_difference'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde85567",
   "metadata": {},
   "source": [
    "`stock_diff_monthly` displays the monthly price change off the Apple-stock."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839af40e",
   "metadata": {},
   "source": [
    "### Calculate weekly change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1e9d12",
   "metadata": {},
   "source": [
    "Measurre the weekly change of the stock price. This will be used later when comparing the weekly price change and weekly sentiment score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414d15b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_open_prize=stock_data.groupby(pd.Grouper(key='Date', freq='W')).first()['Open']\n",
    "weekly_close_price=stock_data.groupby(pd.Grouper(key='Date', freq='W')).last()['Close']\n",
    "\n",
    "stock_diff_weekly=pd.DataFrame(((weekly_close_price-weekly_open_prize)/weekly_open_prize)*100, columns=['Price_difference'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235b42e2",
   "metadata": {},
   "source": [
    "`stock_diff_weekly` displays the weekly price change off the Apple-stock."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b689775",
   "metadata": {},
   "source": [
    "## Preprocessing of data from sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7b9e02",
   "metadata": {},
   "source": [
    "#### Load the sentiment score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e86e5ce",
   "metadata": {},
   "source": [
    "The sentiment score calculated in `sentiment_analysis.ipynb`is loaded to be used later in addition to the stock prices already loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37579fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df_section_stock = pd.read_csv('./data/sentiment_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c0cdd6",
   "metadata": {},
   "source": [
    "### Monthly sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7211c57e",
   "metadata": {},
   "source": [
    "We use the same method as displayed in `sentiment_analysis.ipynb`to calculate the monthly sentiment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662c8740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on how much they contribute increase size of scatter plot circle\n",
    "agg_month_sentiment = []\n",
    "agg_month_dates = []\n",
    "\n",
    "for index, (idx, row) in enumerate(sentiment_df_section_stock.iterrows()):\n",
    "    date = row['date_clean']\n",
    "    if index == 0:\n",
    "        current_month = date[:7]\n",
    "        current_size = 0\n",
    "\n",
    "    tmp_month = current_month\n",
    "    current_month = date[:7] # the seven first digits of the date\n",
    "    if tmp_month != current_month:\n",
    "        agg_month_sentiment.append(current_size)\n",
    "        current_size = 0\n",
    "        agg_month_dates.append(current_month)\n",
    "\n",
    "    # updates weighted aggregated value of positive and negative\n",
    "    current_size += row['compound']*row['numOccurrences']\n",
    "\n",
    "result_values = []\n",
    "result_mon = []\n",
    "for mon, agg in zip(agg_month_dates, agg_month_sentiment):\n",
    "    result_values.append(agg)\n",
    "    result_mon.append(mon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242d3e26",
   "metadata": {},
   "source": [
    "We create a monthly aggregated sentiment dataframe that will be useful later when merging stock data and sentiment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb40289",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_monthly = pd.DataFrame(zip(result_mon, result_values), columns =['Date', 'Sentiment_score'])\n",
    "sentiment_monthly['Date'] = sentiment_monthly['Date'].apply(lambda x: dt.datetime.strptime(x[:10], '%Y-%m'))\n",
    "sentiment_monthly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa15221",
   "metadata": {},
   "source": [
    "### Weekly sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e743dec",
   "metadata": {},
   "source": [
    "To make it easier to comapre the weekly sentiment to the weekly stock price, we remove the first four days of the sentiment data. This makes it easier to merge the two dataframes later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ca6a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df2=sentiment_df_section_stock.copy()\n",
    "sentiment_df2=sentiment_df2[~(sentiment_df2['date_clean'] < '2015-01-04')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a4de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_week_sentiment_weekly = []\n",
    "agg_week_dates_weekly = []\n",
    "day_counter_weekly = 0\n",
    "for index, (idx, row) in enumerate(sentiment_df2.iterrows()):\n",
    "    date = row['date_clean']\n",
    "    if index == 0:\n",
    "        current_day_weekly = date[:10]\n",
    "        current_size_weekly = 0\n",
    "\n",
    "\n",
    "    tmp_day_weekly = current_day_weekly\n",
    "    current_day_weekly = date[:10] # the seven first digits of the date\n",
    "    if tmp_day_weekly != current_day_weekly:\n",
    "        day_counter_weekly += 1\n",
    "    if day_counter_weekly == 7:\n",
    "        agg_week_sentiment_weekly.append(current_size_weekly)\n",
    "        agg_week_dates_weekly.append(current_day_weekly)\n",
    "        current_size_weekly = 0\n",
    "        day_counter_weekly = 0\n",
    "\n",
    "    # updates weighted aggregated value of positive and negative\n",
    "    current_size_weekly += row['compound']*row['numOccurrences']\n",
    "\n",
    "\n",
    "result_week_values = []\n",
    "result_week = []\n",
    "for week, agg in zip(agg_week_dates_weekly, agg_week_sentiment_weekly):\n",
    "    result_week_values.append(agg)\n",
    "    result_week.append(week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb9fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_weekly=pd.DataFrame(zip(result_week, result_week_values), columns =['Date', 'Sentiment_score'])\n",
    "sentiment_weekly['Date'] = pd.to_datetime(sentiment_weekly['Date'])\n",
    "\n",
    "sentiment_weekly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b64880",
   "metadata": {},
   "source": [
    "## Correlation between stock and sentiment (Months)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a2a685",
   "metadata": {},
   "source": [
    "We have now preprocessed the data and can start investigate if there exists any relationship between the stock price and the sentiment. We start by looking at the difference in monthly stock prices and compare it to the aggregated sentiment score for the same month."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea80d4",
   "metadata": {},
   "source": [
    "### Stock price and sentiment for the current month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4029f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_stock_sentiment, (ax1_stock_sentiment, ax2_stock_sentiment) = plt.subplots(2, 1, sharex=True, figsize=(20,10))\n",
    "ax1_stock_sentiment.plot(stock_data_weekly_average.Date, stock_data_weekly_average.Close, label='Apple stock')\n",
    "ax1_stock_sentiment.set_ylabel('Stock closing price ($)', size=18)\n",
    "ax1_stock_sentiment.scatter(apple_event_and_stock_data.Date, apple_event_and_stock_data.Close, c='r', s=100, zorder=3, label='Apple events')\n",
    "ax1_stock_sentiment.scatter(apple_earnings_and_stock_data.Date, apple_earnings_and_stock_data.Close, c='g', s=100, zorder=3, label='Apple earnings date')\n",
    "ax1_stock_sentiment.legend(prop={'size': 25})\n",
    "\n",
    "ax2_stock_sentiment.plot(sentiment_monthly.Date, sentiment_monthly.Sentiment_score, color='green')\n",
    "ax2_stock_sentiment.set_ylabel('Aggregated sentiment intensity score', fontsize=18)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8430868",
   "metadata": {},
   "source": [
    " By just looking at the graphs, it is hard to tell if the sentiment and the stock price is related in any way. The `sentiment_monthly` use the first date of the next month to calculate sentiment, while `stock_diff_monthly` use the last day of this month used to calculate stock difference as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d5254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sentiment_monthly: \" + str(sentiment_monthly.Date[0]) + \", stock_diff_monthly: \" + str(stock_diff_monthly.index[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45838e2",
   "metadata": {},
   "source": [
    "We substract one day from the `sentiment_monthly` such that we easily can merge the two dataframes together based on the `Date`-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa6889",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_monthly['Date']=sentiment_monthly['Date']-DateOffset(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4664df6",
   "metadata": {},
   "source": [
    " We therefore tries to merge the two dataframes by the `Date`-column to further explore the relationships between the sentiment and price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa2427",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price_sentiment_monthly = pd.merge(stock_diff_monthly, sentiment_monthly, how='inner', on = 'Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c30a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price_sentiment_monthly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152ebf3c",
   "metadata": {},
   "source": [
    "We then plot the development of the stock price and the development of the sentiment to see if we see any obvious relationships between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998189df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\", rc={\"axes.spines.right\": False, \"axes.spines.top\": False},\n",
    "              font_scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407985d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_stock_sentiment_2, ax_stock_sentiment_2=plt.subplots(figsize=(20,10))\n",
    "ax_stock_sentiment_2=sns.regplot(x=stock_price_sentiment_monthly.Price_difference, y=stock_price_sentiment_monthly.Sentiment_score)\n",
    "ax_stock_sentiment_2.set_xlabel('Price difference from start to end of a month (%)', size=18)\n",
    "ax_stock_sentiment_2.set_ylabel('Sentiment score for current month', size=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07545f8",
   "metadata": {},
   "source": [
    "We see a slightly positive relationship by looking at the correlation between the stock price and the sentiment score. From this, it might seem that it is more likely to rise in months where the Apple stock gets a lot of positive media attention. However, these findings are not very useful when predicting future stock prices because we would not know the sentiment before the end of the month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5f999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_coefficient, p_value=pearsonr(stock_price_sentiment_monthly.Sentiment_score, stock_price_sentiment_monthly.Price_difference)\n",
    "\n",
    "print(f\"Between monthly sentiment and monthly price difference: Correlation coefficient: {correlation_coefficient} | p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0f518f",
   "metadata": {},
   "source": [
    "With a p-value of $0.40$, it is almost random if a stock will go up or down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dd18d4",
   "metadata": {},
   "source": [
    "### Stock price for current month and sentiment from previous month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7023ef02",
   "metadata": {},
   "source": [
    "We now want to see if there exists a correlation between the previous month sentiment score and this months stock price. This is because such a correlation might help us say something about the most likely future development of the stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832a7e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_previous_month=sentiment_monthly.copy()\n",
    "\n",
    "#Shifts the date one month back in time\n",
    "sentiment_previous_month['Date']=sentiment_previous_month['Date']-DateOffset(months=1, days=5)+MonthEnd(1)\n",
    "\n",
    "#Merge the stock_diff_monthly with the sentiment_previous_month. Each row represent the sentiment for previous month and the stock_price for this month\n",
    "stock_sentiment_previous_month=pd.merge(stock_diff_monthly, sentiment_previous_month, how='inner', on = 'Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5268c4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_stock_sentiment_3, ax_stock_sentiment_3=plt.subplots(figsize=(20,10))\n",
    "sns.regplot(x=stock_sentiment_previous_month.Price_difference, y=stock_sentiment_previous_month.Sentiment_score)\n",
    "ax_stock_sentiment_3.set_xlabel('Price difference from start to end of month (%)', size=18)\n",
    "ax_stock_sentiment_3.set_ylabel('Sentiment score for previous month', size=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b2b701",
   "metadata": {},
   "source": [
    "From the graph it might seem to be a negative relationship between the sentiment for the previous month and the price for this month. Based on this we can expect that a stock is more likely to increase in price following a month where we have seen a low sentiment score and vice versa. We check how likely that this relationship exists using a t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b707e4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_coefficient, p_value=pearsonr(stock_sentiment_previous_month.Sentiment_score, stock_sentiment_previous_month.Price_difference)\n",
    "\n",
    "print(f\"Correlation between monthly price difference and monthly sentiment: Correlation coefficient: {correlation_coefficient} | p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255a61cb",
   "metadata": {},
   "source": [
    "The test also here shows no statistically significant relationship between the prices of this month and the sentiment score from last month. We are thus still without findings when looking for a correlation between the quotes and the stock price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2b72d8",
   "metadata": {},
   "source": [
    "#### Boxplot of the findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1421f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_sentiment_previous_month.loc[stock_sentiment_previous_month['Price_difference']<=-15, 'Category'] = 1\n",
    "stock_sentiment_previous_month.loc[stock_sentiment_previous_month['Price_difference'].between(-15,-10), 'Category'] = 2\n",
    "stock_sentiment_previous_month.loc[stock_sentiment_previous_month['Price_difference'].between(-10,-5), 'Category'] = 3\n",
    "stock_sentiment_previous_month.loc[stock_sentiment_previous_month['Price_difference'].between(-5,0), 'Category'] = 4\n",
    "stock_sentiment_previous_month.loc[stock_sentiment_previous_month['Price_difference'].between(0,5), 'Category'] = 5\n",
    "stock_sentiment_previous_month.loc[stock_sentiment_previous_month['Price_difference'].between(5,10), 'Category'] = 6\n",
    "stock_sentiment_previous_month.loc[stock_sentiment_previous_month['Price_difference'].between(10,15), 'Category'] = 7\n",
    "stock_sentiment_previous_month.loc[stock_sentiment_previous_month['Price_difference']>=15, 'Category'] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\", rc={\"axes.spines.right\": False, \"axes.spines.top\": False},\n",
    "              font_scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f5268",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_stock_sentiment_4, ax_stock_sentiment_4 = plt.subplots(figsize=(15,8))\n",
    "plot=sns.boxplot(x='Category', y='Sentiment_score', data=stock_sentiment_previous_month, hue='Category', saturation=0.5)\n",
    "plt.legend([],[], frameon=False)           \n",
    "ax_stock_sentiment_4.set_ylabel('Sentiment score')\n",
    "ax_stock_sentiment_4.set_xlabel('Price difference')\n",
    "positions = (0, 1, 2, 3, 4, 5, 6,)\n",
    "labels = ('x < -15', '-15 < x < -10', '-10 < x < -5', '-5 < x < 0', '0 < x < 5', '5 < x < 10', '10 < x < 15')\n",
    "plt.xticks(positions, labels, rotation=40);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cfe455",
   "metadata": {},
   "source": [
    "### Stock price for current month and sentiment for following month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbcf5c6",
   "metadata": {},
   "source": [
    "Another interesting question to look into is if a stock rises more before it gets all the media i.e. that people just hear the good news when it is already to late to buy the stock. Therefore we want to see the correlation between the stock price and its sentiment score the following month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c10890",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_following_month=sentiment_monthly.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de1f280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shifts all dates to the following month\n",
    "sentiment_following_month['Date']=sentiment_following_month['Date']+DateOffset(days=5)+MonthEnd(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7be31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_sentiment_following_month=pd.merge(stock_diff_monthly, sentiment_following_month, how='inner', on = 'Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca0bd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\", rc={\"axes.spines.right\": False, \"axes.spines.top\": False},\n",
    "              font_scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eddc568",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_stock_sentiment_5, ax_stock_sentiment_5=plt.subplots(figsize=(20,10))\n",
    "ax_stock_sentiment_5=sns.regplot(x=stock_sentiment_following_month.Price_difference, y=stock_sentiment_following_month.Sentiment_score)\n",
    "ax_stock_sentiment_5.set_xlabel('Price difference from start to end of month (%)', size=18)\n",
    "ax_stock_sentiment_5.set_ylabel('Sentiment score for next month', size=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8940062",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_coefficient, p_value=pearsonr(stock_sentiment_following_month.Sentiment_score, stock_sentiment_following_month.Price_difference)\n",
    "print(f\"Between monthly price difference and sentiment following week: Correlation coefficient: {correlation_coefficient} | p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff0e7a",
   "metadata": {},
   "source": [
    "Here as well, we see no relationship between price and sentiment score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093f3caa",
   "metadata": {},
   "source": [
    "## Correlation between stock and sentiment (weeks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ae2151",
   "metadata": {},
   "source": [
    "So far, we have no results of interest. Therefore, we want to narrow the periods to see if they give any better results than above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dba256",
   "metadata": {},
   "source": [
    "### Stock price and sentiment for current week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee02aff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_sentiment_weekly = pd.merge(stock_diff_weekly, sentiment_weekly, how='inner', on = 'Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58acdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_coefficient, p_value=pearsonr(stock_sentiment_weekly.Sentiment_score, stock_sentiment_weekly.Price_difference)\n",
    "print(f\"Correlation between monthly price difference and monthly sentiment: Correlation coefficient: {correlation_coefficient} | p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3558cb9",
   "metadata": {},
   "source": [
    "### Stock price for current week and sentiment from previous week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5060942",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_previous_week=sentiment_weekly.copy()\n",
    "\n",
    "#Shifts the date one week back in time\n",
    "sentiment_previous_week['Date']=sentiment_previous_week['Date']-DateOffset(days=7)\n",
    "\n",
    "#Merge the sentiment_previous_week with the stock_diff_weekly. Each row represent the sentiment for previous month and the stock_price for this month\n",
    "stock_sentiment_previous_week=pd.merge(stock_diff_weekly, sentiment_previous_week, how='inner', on = 'Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d87389",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_coefficient, p_value=pearsonr(stock_sentiment_previous_week.Sentiment_score, stock_sentiment_previous_week.Price_difference)\n",
    "print(f\"Correlation between weekly price difference and sentiment score the previous week: Correlation coefficient: {correlation_coefficient} | p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc10bee",
   "metadata": {},
   "source": [
    "### Stock price for current week and sentiment for following week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fc4780",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_following_week=sentiment_weekly.copy()\n",
    "\n",
    "#Shifts the date one week back in time\n",
    "sentiment_following_week['Date']=sentiment_following_week['Date']+DateOffset(days=7)\n",
    "\n",
    "#Merge the sentiment_previous_week with the stock_diff_weekly. Each row represent the sentiment for previous month and the stock_price for this month\n",
    "stock_sentiment_following_week=pd.merge(stock_diff_weekly, sentiment_following_week, how='inner', on = 'Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5691967",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_coefficient, p_value=pearsonr(stock_sentiment_following_week.Sentiment_score, stock_sentiment_following_week.Price_difference)\n",
    "print(f\"Correlation between weekly price difference and sentiment score the following week: Correlation coefficient: {correlation_coefficient} | p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae0179b",
   "metadata": {},
   "source": [
    "As we can see from the p-value above, there is a significant correlation between the stock price and its sentiment the following week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c3941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_stock_sentiment_6, ax_stock_sentiment_6=plt.subplots(figsize=(20,10))\n",
    "ax_stock_sentiment_6=sns.regplot(x=stock_sentiment_following_week.Price_difference, y=stock_sentiment_following_week.Sentiment_score)\n",
    "ax_stock_sentiment_6.set_xlabel('Price difference from start to end of month (%)', size=18)\n",
    "ax_stock_sentiment_6.set_ylabel('Sentiment score for next month', size=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f20473",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a4e43d",
   "metadata": {},
   "source": [
    "After looking at the weekly and the monthly values of the price change and the sentiment score, we found no statistically significant correlation that can be used to predict the stock price. We found that the stock price might be used to predict the sentiment for the following week. However, this is not very useful when predicting stock prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebfcbdd",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "# SECTION 4: Event analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427bc073",
   "metadata": {},
   "source": [
    "## Apple product launch analysis\n",
    "Loading the data and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fced32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_analysis = pd.read_csv(\"data/quotes-2020-apple-filter.csv\",sep=\";\")\n",
    "df_event_analysis = df_event_analysis.append(pd.read_csv(\"data/quotes-2019-apple-filter.csv\",sep=\";\"))\n",
    "df_event_analysis = df_event_analysis.append(pd.read_csv(\"data/quotes-2018-apple-filter.csv\",sep=\";\"))\n",
    "df_event_analysis = df_event_analysis.append(pd.read_csv(\"data/quotes-2017-apple-filter.csv\",sep=\";\"))\n",
    "df_event_analysis = df_event_analysis.append(pd.read_csv(\"data/quotes-2016-apple-filter.csv\",sep=\";\"))\n",
    "df_event_analysis = df_event_analysis.append(pd.read_csv(\"data/quotes-2015-apple-filter.csv\",sep=\";\"))\n",
    "\n",
    "#List of dates for the apple events\n",
    "apple_event_dates_str=[\"2015-03-09\",\"2015-06-10\",\"2015-09-09\",\n",
    "\"2016-03-21\",\"2016-06-15\",\"2016-09-07\", \"2016-10-27\",\n",
    "\"2017-06-07\", \"2017-09-12\",\n",
    "\"2018-03-27\",\"2018-06-06\", \"2018-09-12\", \"2018-10-30\",\n",
    "\"2019-03-25\",\"2019-06-05\",\"2019-09-10\",\"2019-12-02\",\n",
    "\"2020-06-24\",\"2020-09-15\",\"2020-10-13\",\"2020-11-10\"]\n",
    "\n",
    "# load sentiment dataframe\n",
    "sentiment_df_event_analysis = pd.read_csv('./data/sentiment_df.csv')\n",
    "\n",
    "#apple_event_dates_pd = pd.DataFrame({'Date':[dt.datetime.strptime(date, \"%Y-%m-%d\").date() for date in apple_event_dates_str]})\n",
    "apple_event_dates=[dt.datetime.strptime(date, \"%Y-%m-%d\").date() for date in apple_event_dates_str]\n",
    "\n",
    "# Clean the date column, such that it only contains date information and not timestamp\n",
    "df_event_analysis['date_clean'] = df_event_analysis.apply(lambda x: x['date'][:10],axis=1)\n",
    "df_event_analysis['date_clean_datetime'] = df_event_analysis['date_clean'].apply(lambda x: dt.datetime.strptime(x[:10], '%Y-%m-%d').date())\n",
    "\n",
    "# Clean the date column, such that it only contains date information and not timestamp\n",
    "df_event_analysis['date_clean'] = df_event_analysis.apply(lambda x: x['date'][:10],axis=1)\n",
    "df_event_analysis['date_clean_datetime'] = df_event_analysis['date_clean'].apply(lambda x: dt.datetime.strptime(x[:10], '%Y-%m-%d').date())\n",
    "sentiment_df_event_analysis['date_clean_datetime'] = sentiment_df_event_analysis['date_clean'].apply(lambda x: dt.datetime.strptime(x[:10], '%Y-%m-%d').date())\n",
    "\n",
    "\n",
    "# Drop outlier\n",
    "df_event_analysis = df_event_analysis.drop(df_event_analysis.loc[df_event_analysis.numOccurrences == 39978].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce37fc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for quote in sentiment_df_event_analysis[sentiment_df_event_analysis[\"date_clean\"].isin(apple_event_dates_str)][\"quotation\"].sample(10):\n",
    "    print(quote)\n",
    "    print('------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8e1c5a",
   "metadata": {},
   "source": [
    "Let's start by looking at how the number of apple related citations are related to product launches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acfe002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice the data frame \n",
    "time = df_event_analysis.groupby(['date_clean']).sum().index\n",
    "quote_num = df_event_analysis.groupby(['date_clean']).sum()['numOccurrences']\n",
    "fig_event_analysis, ax_event_analysis = plt.subplots(figsize=(18,8))\n",
    "ax_event_analysis.plot(time,quote_num, label= \"Quote occurrences\")\n",
    "ax_event_analysis.set_xlabel(\"dates\",size=15)\n",
    "ax_event_analysis.scatter(time[time.isin(apple_event_dates_str)],quote_num[time.isin(apple_event_dates_str)], c=\"r\", s=100, zorder=3, label='Apple events')\n",
    "plt.xticks(time[::30],rotation=90)\n",
    "ax_event_analysis.set_ylabel(\"Quotation occurrences\",size=15)\n",
    "ax_event_analysis.set_title(\"Number of daily Apple-related Citations 2015 through 2020\",size=18)\n",
    "ax_event_analysis.legend(prop={'size': 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7411c8fd",
   "metadata": {},
   "source": [
    "**Quantifying the media attention**\n",
    "\n",
    "To quantify the media attention for a given Apple event, the following metrics are formulated:\n",
    "- **Attention**: The number of weekly quote occurrences.\n",
    "- **Baseline attention**: Average attention throughout the time period.\n",
    "- **Event attention**: Avg. of quote occurrences through event week, prior and post week.\n",
    "- **Event attention increase %**: (Event attention - Baseline attention) / Baseline attention.\n",
    "\n",
    "\n",
    "To be able to compute these metrics, as well as make further analysis possible, we create the following dataframe:\n",
    "\n",
    "*Columns:* \\\n",
    "Event-date, attention_week3_pre, attention_week2_pre, attention_week1_pre, attention_week0, attention_week1_post, attention_week2_post, attention_week3_post\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events = pd.DataFrame()\n",
    "events = []\n",
    "for date in apple_event_dates:\n",
    "    if (date.year == 2019 and date.month < 12) or date.year < 2020:\n",
    "        events.append(date)\n",
    "df_events['event_date'] = events\n",
    "\n",
    "# group data frame by week and year to get weekly attention numbers\n",
    "df_event_analysis['week'] = df_event_analysis.apply(lambda x: str(x['date_clean_datetime'].isocalendar()[1]), axis=1)\n",
    "df_event_analysis['year'] = df_event_analysis.apply(lambda x: str(x['date_clean_datetime'].isocalendar()[0]), axis=1)\n",
    "df_event_analysis['yearweek'] = df_event_analysis['week'] + df_event_analysis['year']\n",
    "df_attention = df_event_analysis.groupby(['year','week']).sum().reset_index()\n",
    "\n",
    "\n",
    "# group sentiment data frame\n",
    "sentiment_df_event_analysis['week'] = sentiment_df_event_analysis.apply(lambda x: str(x['date_clean_datetime'].isocalendar()[1]), axis=1)\n",
    "sentiment_df_event_analysis['year'] = sentiment_df_event_analysis.apply(lambda x: str(x['date_clean_datetime'].isocalendar()[0]), axis=1)\n",
    "sentiment_week = sentiment_df_event_analysis.groupby(['year','week']).mean().reset_index()\n",
    "\n",
    "def attention_query(date, num_weeks):\n",
    "    \"adds weeks to date and returns the attention the corresponding week and year\"\n",
    "    new_date = date + dt.timedelta(weeks=num_weeks)\n",
    "    year = str(new_date.isocalendar()[0])\n",
    "    week = str(new_date.isocalendar()[1])\n",
    "    count = df_attention.loc[(df_attention.week == week) & (df_attention.year == year)].numOccurrences.item()\n",
    "    return count\n",
    "\n",
    "def sentiment_query(date, num_weeks):\n",
    "    \"adds weeks to date and returns the sentiment the corresponding week and year\"\n",
    "    new_date = date + dt.timedelta(weeks=num_weeks)\n",
    "    year = str(new_date.isocalendar()[0])\n",
    "    week = str(new_date.isocalendar()[1])\n",
    "    sentiment = sentiment_week.loc[(sentiment_week.week == week) & (sentiment_week.year == year)].compound.item()\n",
    "    return sentiment\n",
    "\n",
    "# Create columns for all events\n",
    "for i in [-3,-2,-1,0,1,2,3]:\n",
    "    df_events[f\"\"\"attention_week{i}{'_pre' if i < 0 else '_post' if i > 0 else ''}\"\"\"] = df_events.apply(lambda x: attention_query(x['event_date'], i),axis=1)\n",
    "    df_events[f\"\"\"sentiment_week{i}{'_pre' if i < 0 else '_post' if i > 0 else ''}\"\"\"] = df_events.apply(lambda x: sentiment_query(x['event_date'], i),axis=1)\n",
    "\n",
    "attention_columns =  [ 'attention_week-3_pre', 'attention_week-2_pre',  'attention_week-1_pre', 'attention_week0','attention_week1_post', 'attention_week2_post','attention_week3_post']\n",
    "sentiment_columns = ['sentiment_week-3_pre','sentiment_week-2_pre','sentiment_week-1_pre','sentiment_week0','sentiment_week1_post','sentiment_week2_post', 'sentiment_week3_post']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f844eb",
   "metadata": {},
   "source": [
    "Let's visualize the typical distribution of media attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f492d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(18,8))\n",
    "sns.set(font_scale=2)\n",
    "sns.set_theme(style=\"white\", rc={\"axes.spines.right\": False, \"axes.spines.top\": False},\n",
    "              font_scale=2)\n",
    "ax = sns.boxplot(data=df_events[attention_columns],\n",
    "                     saturation=0.5)\n",
    "#ax.set_title(\"Media attention before, during and after an Apple event\")\n",
    "ax.set_ylabel(\"Apple related quotes\")\n",
    "positions = (0,1, 2, 3, 4, 5, 6)\n",
    "labels = (\"3 weeks prior\", \"2 weeks prior\", \"1 week prior\",\"Event week\", \"1 week after\", \"2 weeks after\", \"3 weeks after\")\n",
    "plt.xticks(positions, labels, rotation=22.5);\n",
    "#sns.despine(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae208743",
   "metadata": {},
   "source": [
    "Looking at the medians, we see that the media attention rises until the week of the event where it peaks and then gradually declines in the following weeks. This shows that Apple is able to build anticipation in the media when launching new products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca92dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(18,8))\n",
    "ax = sns.boxplot(data=df_events[sentiment_columns],\n",
    "                     saturation=0.5)\n",
    "ax.set_title(\"Sentiment of quotes before, during and after an Apple event\")\n",
    "ax.set_ylabel(\"Sentiment of Apple related quotes\")\n",
    "positions = (0,1, 2, 3, 4, 5, 6)\n",
    "labels = (\"3 weeks prior\", \"2 weeks prior\", \"1 week prior\",\"Event week\", \"1 week after\", \"2 weeks after\", \"3 weeks after\")\n",
    "plt.xticks(positions, labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef358188",
   "metadata": {},
   "source": [
    "Above we see a boxplot of the average sentiment of the quotes in the weeks prior to and after an apple event. The sentiment value ranges from -1 to 1, with 1 being very positive and -1 being very negative. Across all weeks, we see that the sentiment of the quotes typically lies around 0.2, indicidating a slight positive sentiment in the quotes but mostly neutral. Although the medians do not differ much, the median of the event week is still the highest value. From this we can conclude that Apple generally has a stable reputation in the media, with slightly positive quotes made about them on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f644ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute baseline\n",
    "baseline = df_attention.numOccurrences.mean()\n",
    "df_events['baseline'] = baseline\n",
    "\n",
    "# Compute average of attention in the weeks prior and after an event \n",
    "columns = ['attention_week-3_pre', 'attention_week-2_pre','attention_week-1_pre', 'attention_week0', 'attention_week1_post', 'attention_week2_post', 'attention_week3_post']\n",
    "df_events['event_attention'] = df_events[columns].mean(axis=1)\n",
    "df_events['attention_increase'] =  100 * (df_events.event_attention - df_events.baseline) / df_events.baseline\n",
    "df_events['color'] = df_events.apply(lambda x: 'indianred' if x['attention_increase'] < 0  else 'seagreen',axis=1)\n",
    "df_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ab6c2b",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea37254",
   "metadata": {},
   "source": [
    "Now looking at how different apple events have deviated from the attention average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ad7cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(18,8))\n",
    "plt.bar(x=apple_event_dates_str[:17],height=df_events.attention_increase,color=df_events.color)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Event dates\")\n",
    "plt.ylabel(\"Attention increase from baseline (%)\")\n",
    "plt.title(\"Apple events' deviation from attention baseline\")\n",
    "plt.text(10.9, 10, \"iPhone XS, XR, Apple Watch\", fontsize=14,rotation=90,color=\"black\")\n",
    "plt.text(7.9, 10, \"iPhone 8, iPhone X\", fontsize=14,rotation=90,color=\"white\")\n",
    "plt.text(4.9, 10, \"iPhone 7, AirPods\", fontsize=14,rotation=90,color=\"white\")\n",
    "plt.text(1.9, 10, \"iPhone 6S,\", fontsize=14,rotation=90,color=\"white\")\n",
    "plt.text(1.9, 65, \"iPad Pro/Mini\", fontsize=14,rotation=90,color=\"black\")\n",
    "plt.text(5.9, -90, \"Macbook Pro\", fontsize=14,rotation=90,color=\"white\");\n",
    "plt.text(2.9, -90, \"iPhone SE, iPad Pro\", fontsize=14,rotation=90,color=\"white\");\n",
    "plt.text(3.9, -50, \"Software\", fontsize=14,rotation=90,color=\"white\");\n",
    "plt.text(12.9, -100, \"Apple TV+/Card/Arcade\", fontsize=14,rotation=90,color=\"black\");\n",
    "plt.text(14.9, -105, \"iPhone 11, Apple Watch\", fontsize=14,rotation=90,color=\"black\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2491dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(12,18))\n",
    "plt.barh(y=apple_event_dates_str[:17],width=df_events.attention_increase,color=df_events.color)\n",
    "#plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Event dates\")\n",
    "plt.ylabel(\"Attention increase from baseline (%)\")\n",
    "plt.title(\"Apple events' deviation from attention baseline\")\n",
    "plt.text(10, 10.9, \"iPhone XS, XR, Apple Watch\", fontsize=14, color=\"black\")\n",
    "plt.text(10, 7.9, \"iPhone 8, iPhone X\", fontsize=14,color=\"white\")\n",
    "plt.text(10, 4.9, \"iPhone 7, AirPods\", fontsize=14,color=\"white\")\n",
    "plt.text(10, 1.9, \"iPhone 6S,\", fontsize=14,color=\"white\")\n",
    "plt.text(65, 1.9, \"iPad Pro/Mini\", fontsize=14,color=\"black\")\n",
    "plt.text(-90, 5.9, \"Macbook Pro\", fontsize=14,color=\"white\");\n",
    "plt.text(-90, 2.9, \"iPhone SE, iPad Pro\", fontsize=14,color=\"white\");\n",
    "plt.text(-50, 3.9, \"Software\", fontsize=14,color=\"white\");\n",
    "plt.text(-100, 12.9, \"Apple TV+/Card/Arcade\", fontsize=14,color=\"black\");\n",
    "plt.text(-105, 14.9, \"iPhone 11, Apple Watch\", fontsize=14,color=\"black\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a2880a",
   "metadata": {},
   "source": [
    "## Does the sentiment of quarter earning announcements differ from product launches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_earnings=pd.read_excel('./data/Earnings_Apple.xlsx')\n",
    "\n",
    "#Remove the unnecessary rows in the quarterly_earnings\n",
    "quarterly_earnings=quarterly_earnings.drop(index=[0,1,2,3,4,5,6,7])\n",
    "\n",
    "#Change the format of the Earnings Date column\n",
    "quarterly_earnings[\"Earnings Date\"]=quarterly_earnings[\"Earnings Date\"].map(lambda x: x.replace(\", 12 AMEST\", \"\").replace(\",\", \"\"))\n",
    "quarterly_earnings[\"Earnings Date\"]=quarterly_earnings[\"Earnings Date\"].map(lambda x: dt.datetime.strptime(x, \"%b %d %Y\"))\n",
    "\n",
    "#Clean surprise factor\\n\",\n",
    "quarterly_earnings[\"Surprise(%)\"] = quarterly_earnings[\"Surprise(%)\"].map(lambda x: -1*x[1:] if x[0] == '-' else x[1:])\n",
    "\n",
    "# Remove dates before 2020\\n\",\n",
    "to_drop = []\n",
    "for i, date in enumerate(quarterly_earnings[\"Earnings Date\"]):\n",
    "    if date.year > 2019:\n",
    "        to_drop.append(i+8)\n",
    "quarterly_earnings = quarterly_earnings.drop(index=to_drop,axis=0)\n",
    "\n",
    "# Add sentiment columns\\n\"\n",
    "for i in [-3,-2,-1,0,1,2,3]:\n",
    "    quarterly_earnings[f\"\"\"sentiment_week{i}{'_pre' if i < 0 else '_post' if i > 0 else ''}\"\"\"] = quarterly_earnings.apply(lambda x: sentiment_query(x['Earnings Date'], i),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590a69f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = quarterly_earnings[sentiment_columns].mean().tolist()\n",
    "b = df_events[sentiment_columns].mean().tolist()\n",
    "\n",
    "# plot data in grouped manner of bar type\\n\",\n",
    "plt.subplots(figsize=(20,10))\n",
    "x = np.arange(7)\n",
    "width = 0.4\n",
    "plt.bar(x-0.2, a, width,color='darkslategrey')\n",
    "plt.bar(x+0.2, b, width,color='cornflowerblue')\n",
    "plt.xticks(x,labels)\n",
    "plt.legend(['Product launches', 'Quarterly earnings announced'])\n",
    "plt.ylabel(\"Mean sentiment of quotes\")\n",
    "plt.title(\"Comparison of mean sentiment for product launches and earning announcements\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4836850",
   "metadata": {},
   "source": [
    "Looking at the plot above, it can be seen that the sentiment of quotes before and after product launches on average are very similar the sentiment of quotes before and after announcements of quarterly earnings. The quotes are, however, generally more positive in the weeks where quarterly earnings are announced than in the weeks where new products are launched. This can be explained by the fact that quarterly reports typically are described with more adjectives, whereas product launches may be more factual and thus neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a6896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b = df_events[sentiment_columns].mean().tolist()\n",
    "\n",
    "# set theme of plot\n",
    "sns.set_theme(style='white', rc={'axes.spines.right': False, 'axes.spines.top': False}, font='Arial', font_scale=2)\n",
    "# plot data in grouped manner of bar type\\n\",\n",
    "plt.subplots(figsize=(20,10))\n",
    "x = np.arange(7)\n",
    "width = 0.5\n",
    "#plt.bar(x-0.2, a, width,color='darkslategrey')\n",
    "plt.bar(x, b, width,color='#800020')\n",
    "plt.xticks(x,labels, rotation=30)\n",
    "#plt.legend(['Product launches', 'Quarterly earnings announced'])\n",
    "plt.ylabel(\"Mean sentiment of quotes\")\n",
    "#plt.title(\"Comparison of mean sentiment for product launches and earning announcements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6bd1c9",
   "metadata": {},
   "source": [
    "From the plot it is clear that the not only the attention, but also the sentiment towards Apple increases during an event week. In addition the plot indicates that the sentiment in the weeks after an event is more positive than the weeks prior. To investigate this we will perform a Mann Whitney U-test, a robust, non-parametric test which measures whether there exist a significant difference between to distributions. \n",
    "\n",
    "Source: [Towards Data Science](https://towardsdatascience.com/intro-to-data-science-part-3-data-analysis-71a566c3a8c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cb9126",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9273633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make numpy representations of weeks prior to event, and posterior to event\n",
    "pre_columns = ['sentiment_week-3_pre', 'sentiment_week-2_pre', 'sentiment_week-1_pre']\n",
    "post_columns = ['sentiment_week3_post', 'sentiment_week2_post', 'sentiment_week1_post']\n",
    "\n",
    "prior_weeks = np.mean(df_events[pre_columns], axis=1)\n",
    "posterior_weeks = np.mean(df_events[post_columns], axis=1)\n",
    "event_week = df_events[\"sentiment_week0\"]\n",
    "\n",
    "# Combine prior and posterior to represent all weeks except event week\n",
    "prior_posterior = np.array([prior_weeks, posterior_weeks]).T\n",
    "\n",
    "# Conduct test between weeks prior and posterior\n",
    "test_result = mannwhitneyu(prior_weeks, posterior_weeks)\n",
    "\n",
    "print(f'Sentiment for the three weeks prior to a launch:')\n",
    "print(f'Mean: {prior_weeks.mean()}, Standard Deviation: {prior_weeks.std()}')\n",
    "print('----------------------------------------------------------------------')\n",
    "print(f'Sentiment for the three weeks posterior to a launch:')\n",
    "print(f'Mean: {posterior_weeks.mean()}, Standard Deviation: {posterior_weeks.std()}')\n",
    "print('----------------------------------------------------------------------')\n",
    "print(f'Mann Whitney U-Test: {test_result}')\n",
    "\n",
    "# Conduct test between event week and all other\n",
    "event_rest = mannwhitneyu(prior_posterior.mean(axis=1), event_week)\n",
    "print('----------------------------------------------------------------------')\n",
    "print(f'Sentiment for event week:')\n",
    "print(f'Mean: {posterior_weeks.mean()}, Standard Deviation: {posterior_weeks.std()}')\n",
    "print('----------------------------------------------------------------------')\n",
    "print(f'Mann Whitney U-Test between event week and all other: {event_rest}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a57c7c",
   "metadata": {},
   "source": [
    "Mann Whitney U-test that the two distributions are equal. The p-value represents the percentages of times we would see a similar (or more extreme) result as our obeservations given this assumption. Since the p-value is ~0.3, by drawing two random samples from the same distribution would yield at least as big difference as observed. This difference is not significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559d4a5c",
   "metadata": {},
   "source": [
    "### Does the sentiment/attention before and after a product launch correlate with changes in the stock price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d42434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentiment change for each of the product launches\n",
    "df_events['sentiment_change'] = df_events[\"sentiment_week1_post\"] - df_events['sentiment_week-1_pre']\n",
    "df_events['attention_change'] = df_events[\"attention_week1_post\"] - df_events['attention_week-1_pre']\n",
    "\n",
    "\n",
    "# Calculate the change in stock price before and after each stock price (average of the week before to the week after)\n",
    "stock_data_events=pd.read_csv('./data/AAPL_2015_to_2020_yahoo_finance.csv')\n",
    "stock_data_events.Date = pd.to_datetime(stock_data_events.Date) - pd.to_timedelta(7, unit='d')\n",
    "stock_data_events = stock_data_events.groupby([pd.Grouper(key='Date', freq='W-MON')]).mean().reset_index().sort_values('Date')\n",
    "stock_data_events['week'] = stock_data_events.apply(lambda x: x['Date'].week,axis=1)\n",
    "stock_data_events['year'] = stock_data_events.apply(lambda x: x['Date'].year,axis=1)\n",
    "\n",
    "def stock_query(date):\n",
    "    \"Given date returns the difference between the stockprice the week before and after said date\"\n",
    "    year = date.isocalendar()[0]\n",
    "    week = date.isocalendar()[1]\n",
    "    stock_start = stock_data_events.loc[(stock_data_events.week == week - 1) & (stock_data_events.year == year)].Open.item()\n",
    "    stock_end = stock_data_events.loc[(stock_data_events.week == week + 1) & (stock_data_events.year == year)].Close.item()\n",
    "\n",
    "    diff = stock_start - stock_end\n",
    "    return diff\n",
    "\n",
    "\n",
    "df_events[\"stock_change\"] = df_events.apply(lambda x: stock_query(x['event_date']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dd734c",
   "metadata": {},
   "source": [
    "Let's see if there is any connection between the change in stock price and sentiment/attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9b6ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(font_scale=1.5, style='whitegrid', font='Arial', rc={\"axes.spines.right\": False, \"axes.spines.top\": False})\n",
    "fig, axes =plt.subplots(1,2,figsize=(20,10))\n",
    "sns.set_theme(style='white', rc={'axes.spines.right': False, 'axes.spines.top': False}, font_scale=2)\n",
    "sns.regplot(ax=axes[0], x=df_events[\"stock_change\"], y=df_events[\"sentiment_change\"])\n",
    "axes[0].set_xlabel('Price difference of stock after product launch ($)', size=25)\n",
    "axes[0].set_ylabel('Difference in sentiment score after product launch ', size=25)\n",
    "sns.regplot(ax=axes[1],x=df_events[\"stock_change\"], y=df_events[\"attention_change\"],color='coral')\n",
    "axes[1].set_xlabel('Price difference of stock after product launch ($)', size=25)\n",
    "axes[1].set_ylabel('Difference in attention score after product launch ', size=25)\n",
    "\n",
    "#fig.suptitle('Does changes in sentiment/attention after a product launch correlate with changes in stock prices?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c293af9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "mod = sm.OLS(df_events[\"stock_change\"],df_events[\"sentiment_change\"])\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdaee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sm.OLS(df_events[\"stock_change\"],df_events[\"attention_change\"])\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33304981",
   "metadata": {},
   "source": [
    "Both regression lines have very low R^2-values indicating a poor fit. To conclude, the change in stock price from the week before an event to the week after an event can neither be explained by the corresponding change in attention level nor the change in sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada9ccf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
