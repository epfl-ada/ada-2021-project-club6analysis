{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "655623ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "import os.path\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import gc\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e21a8cf",
   "metadata": {},
   "source": [
    "### Data preproccessing\n",
    "In this step we will change the compressed json format into csv files for any given year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a8bc3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_chunk(chunk, csv_file_name):\n",
    "    \"\"\" \n",
    "    Explanation:\n",
    "    This function will take in some chunk of data and filter out the not wanter columns\n",
    "    and do some small processing on the type and then write this to a csv file.\n",
    "    \n",
    "    INPUT:\n",
    "        chunk:                  A dataframe chunk\n",
    "        csv_file_name:          Filepath to the resulting csv file\n",
    "        \n",
    "    \"\"\"\n",
    "    #print(chunk.columns)\n",
    "\n",
    "    csv_lines = []\n",
    "\n",
    "    # Changing the dtype of the date from Timestamp to string\n",
    "    chunk['date'] = chunk['date'].astype(\"str\")\n",
    "\n",
    "    for index, row in chunk.iterrows():\n",
    "        current_csv_line = [row['quoteID'], \n",
    "                            row['quotation'], \n",
    "                            row['speaker'], \n",
    "                            row['date'], \n",
    "                            str(row['numOccurrences'])]\n",
    "\n",
    "\n",
    "        # Appending the current csv line to the csv_lines list, so we can add\n",
    "        # chunksize number of lines at the same time\n",
    "        csv_lines.append(current_csv_line)\n",
    "\n",
    "        #with open(csv_file_name, 'a') as file:\n",
    "        #    file.writelines(csv_lines)\n",
    "\n",
    "    with open(csv_file_name, 'a', encoding='UTF8', newline='') as f: \n",
    "        writer = csv.writer(f, delimiter=';')\n",
    "        writer.writerows(csv_lines)\n",
    "\n",
    "def compressed_json_to_csv(json_file_path, csv_file_path, chunksize):\n",
    "    \"\"\" \n",
    "    Explanation:\n",
    "    This function will take in a json_file_path, do some filtering and then write the results\n",
    "    in to the csv_file_name path\n",
    "    \n",
    "    INPUT:\n",
    "        json_file_path:         Filepath to the compressed json file\n",
    "        csv_file_path:          Filepath to the resulting csv file\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the new csv file exists already\n",
    "    if not os.path.isfile(csv_file_path):\n",
    "        f = open(csv_file_path, 'w', encoding='UTF8', newline='')\n",
    "        writer = csv.writer(f, delimiter=';')\n",
    "        writer.writerow(['quoteID', 'quotation', 'speaker', 'date', 'numOccurrences']) \n",
    "        f.close()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    chunk_counter = 0\n",
    "    # Will chunkwise read in lines from the compressed json file\n",
    "    with pd.read_json(json_file_path, lines=True, compression='bz2', chunksize=chunksize) as df_reader:\n",
    "        for chunk in df_reader:\n",
    "            process_json_chunk(chunk, csv_file_path)\n",
    "            \n",
    "            chunk_counter += len(chunk)\n",
    "            print(f'Processed {chunk_counter} rows in {time.time()-start_time}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eedff02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9469966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ab798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68e9ee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_string_to_list(string):\n",
    "    \"\"\" \n",
    "    EXPLANATION:\n",
    "    This function will take a string and turn it into a stemmed list of words\n",
    "    \n",
    "    INPUT:\n",
    "        string:    string of words\n",
    "    \n",
    "    OUTPUT:\n",
    "        return:    list of words that are stemmed\n",
    "    \"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    return [ps.stem(word) for word in string.split()]\n",
    "\n",
    "def stem_list_to_list(arr):\n",
    "    \"\"\" \n",
    "    EXPLANATION:\n",
    "    This function will take a string and turn it into a stemmed list of words\n",
    "    \n",
    "    INPUT:\n",
    "        arr:    list of words that are stemmed\n",
    "    \n",
    "    OUTPUT:\n",
    "        return:    list of words that are stemmed\n",
    "    \"\"\"\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    return [ps.stem(word) for word in arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2fa1f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_chunk(chunk, stem_add_list, stem_check_list, stem_exclude_list, speaker_list, filename):\n",
    "    \"\"\" \n",
    "    EXPLANATION:\n",
    "    This function will filter a chunk of data, based on the stemmed lists, \n",
    "    and write the result to the filename filepath\n",
    "    \n",
    "    INPUT:\n",
    "        chunk:              Chunk of data in a dataframe format\n",
    "        stem_add_list:      Stemmed list of sentences that will NOT be filtered out\n",
    "        stem_check_list:    Stemmed list of sentences that will MAYBE not be filtered out\n",
    "        stem_exlude_list:   Stemmed list of steneces that will be filtered out\n",
    "        filename:           Filepath of where the filtered data will be written to\n",
    "    \"\"\"\n",
    "    \n",
    "    #print(f'Processing chunk with {len(chunk)} rows')\n",
    "    csv_lines = []\n",
    "    \n",
    "    # Iterates over the chunk - row by row\n",
    "    for index, row in chunk.iterrows():\n",
    "        \n",
    "        speaker = row['speaker']\n",
    "        \n",
    "        if speaker in speaker_list:\n",
    "            csv_lines.append(row)\n",
    "        else:\n",
    "            # Only stem the quotations we don't find the speaker\n",
    "            stemmed_quote_list = frozenset(stem_string_to_list(row['quotation']))\n",
    "            #========================================================\n",
    "            # Starting to check if quote contains any must have words\n",
    "            if not stemmed_quote_list.isdisjoint(stem_add_list):\n",
    "                csv_lines.append(row)\n",
    "\n",
    "            #========================================================\n",
    "            # Starting to check if quote contains any could have words\n",
    "            elif (not stemmed_quote_list.isdisjoint(stem_check_list)) and stemmed_quote_list.isdisjoint(stem_exclude_list):\n",
    "                csv_lines.append(row)\n",
    "            \n",
    "    \n",
    "    # Writing all the csv_lines that where not filtered out to the csv file\n",
    "    with open(filename, 'a', encoding='UTF8', newline='') as f: \n",
    "        writer = csv.writer(f, delimiter=';')\n",
    "        writer.writerows(csv_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3904963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out_apple_from_csv(csv_file_path, filtered_csv_file_path, add_list, check_list, exclude_list,speaker_list, chunksize):\n",
    "    \"\"\" \n",
    "    EXPLANATION:\n",
    "    This function will filter out data from a csv_file_path, and write the filtered data to \n",
    "    a new filtered_csv_file_path.\n",
    "    \n",
    "    INPUT:\n",
    "        csv_file_path:             Filepath to where the UNFILTERED data is located\n",
    "        filtered_csv_file_path:    Filepath to where the FILTERED data will be written\n",
    "        stem_add_list:             Stemmed list of sentences that will NOT be filtered out\n",
    "        stem_check_list:           Stemmed list of sentences that will MAYBE not be filtered out\n",
    "        stem_exlude_list:          Stemmed list of steneces that will be filtered out\n",
    "        filename:                  Filepath of where the filtered data will be written to\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup variables\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    \n",
    "    # Checks if all the files are in order\n",
    "    if not os.path.isfile(filtered_csv_file_path):\n",
    "        # Creates a new file it it does not exists\n",
    "        f = open(filtered_csv_file_path, 'w')\n",
    "        writer = csv.writer(f, delimiter=';')\n",
    "        writer.writerow(['quoteID', 'quotation', 'speaker', 'date', 'numOccurrences']) \n",
    "        f.close()\n",
    "    if not os.path.isfile(csv_file_path):\n",
    "        raise Exception(\"You will need to input valid csv_file_path!\")\n",
    "    \n",
    "    # Stemming the lists we will use to filter quoteID;quotation;speaker;date;numOccurrences\n",
    "    stem_add_set = frozenset(stem_list_to_list(add_list))\n",
    "    stem_check_set = frozenset(stem_list_to_list(check_list))\n",
    "    stem_exclude_set = frozenset(stem_list_to_list(exclude_list))\n",
    "    speaker_set = frozenset(speaker_list)\n",
    "    start_time = time.time()\n",
    "    chunk_counter = 0\n",
    "    # Start the iterative processing of the csv files\n",
    "    for chunk in pd.read_csv(csv_file_path, chunksize=chunksize, delimiter=';'):\n",
    "        process_csv_chunk(chunk, stem_add_set, stem_check_set, stem_exclude_set, speaker_set, filtered_csv_file_path)\n",
    "        clear_output()\n",
    "        chunk_counter += len(chunk)\n",
    "        print(f'Processed {chunk_counter} rows in {time.time()-start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ece9729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_list = [\"apple inc\", \"tim cook\", \"steve jobs\", \"iphone\", \"ipad\", \"imac\", \"apple watch\", \"macbook\", \"macbook pro\", \"aapl\", \"mac mini\", \"app store\"]\n",
    "check_list = [\"apple\"]\n",
    "exclude_list = [\"fruit\", 'banana', 'orange', 'strawberries', \"watermelon\", 'lemons', 'blueberries']\n",
    "speaker_list = set([\"Tim Took\", \"Steve Jobs\", \"Katherine Adamas\", \"Jony Ive\", \"Eddy Cue\", \"Craig Frederighi\", \"Greg joswiak\", \"Stella Low\", \"Tor Myhren\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec7b9b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 rows in 137.3291049003601\n",
      "Processed 2000000 rows in 274.09804701805115\n"
     ]
    }
   ],
   "source": [
    "compressed_json_to_csv(\"data/quotes-2018.json.bz2\", \"data/quotes-2018-first-filter.csv\", 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4852dcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1800000 rows in 648.7888848781586\n"
     ]
    }
   ],
   "source": [
    "filter_out_apple_from_csv(\"data/quotes-2018-first-filter.csv\", \n",
    "                          \"data/quotes-2018-apple-filter.csv\", \n",
    "                          add_list, check_list, exclude_list, speaker_list, 200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a1ad67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
