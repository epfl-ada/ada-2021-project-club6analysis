{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "655623ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "import os.path\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import gc\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04201743",
   "metadata": {},
   "source": [
    "### Data preproccessing\n",
    "In this step we will change the compressed json format into csv files for any given year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42431179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_chunk(chunk, csv_file_name):\n",
    "    \"\"\" \n",
    "    Explanation:\n",
    "    This function will take in some chunk of data and filter out the not wanter columns\n",
    "    and do some small processing on the type and then write this to a csv file.\n",
    "    \n",
    "    INPUT:\n",
    "        chunk:                  A dataframe chunk\n",
    "        csv_file_name:          Filepath to the resulting csv file\n",
    "        \n",
    "    \"\"\"\n",
    "    print(f'Processing chunk with {len(chunk)} rows')\n",
    "    #print(chunk.columns)\n",
    "\n",
    "    csv_lines = []\n",
    "\n",
    "    # Changing the dtype of the date from Timestamp to string\n",
    "    chunk['date'] = chunk['date'].astype(\"str\")\n",
    "\n",
    "    for index, row in chunk.iterrows():\n",
    "        current_csv_line = [row['quoteID'], \n",
    "                            row['quotation'], \n",
    "                            row['speaker'], \n",
    "                            row['date'], \n",
    "                            str(row['numOccurrences'])]\n",
    "\n",
    "\n",
    "        # Appending the current csv line to the csv_lines list, so we can add\n",
    "        # chunksize number of lines at the same time\n",
    "        csv_lines.append(current_csv_line)\n",
    "\n",
    "        #with open(csv_file_name, 'a') as file:\n",
    "        #    file.writelines(csv_lines)\n",
    "\n",
    "    with open(csv_file_name, 'a', encoding='UTF8', newline='') as f: \n",
    "        writer = csv.writer(f, delimiter=';')\n",
    "        writer.writerows(csv_lines)\n",
    "\n",
    "def compressed_json_to_csv(json_file_path, csv_file_path):\n",
    "    \"\"\" \n",
    "    Explanation:\n",
    "    This function will take in a json_file_path, do some filtering and then write the results\n",
    "    in to the csv_file_name path\n",
    "    \n",
    "    INPUT:\n",
    "        json_file_path:         Filepath to the compressed json file\n",
    "        csv_file_path:          Filepath to the resulting csv file\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the new csv file exists already\n",
    "    if not os.path.isfile(csv_file_path):\n",
    "        f = open(csv_file_path, 'w', encoding='UTF8', newline='')\n",
    "        writer = csv.writer(f, delimiter=';')\n",
    "        writer.writerow(['quoteID', 'quotation', 'speaker', 'date', 'numOccurrences']) \n",
    "        f.close()\n",
    "    \n",
    "    \n",
    "    # Will chunkwise read in lines from the compressed json file\n",
    "    with pd.read_json(json_file_path, lines=True, compression='bz2', chunksize=1000000) as df_reader:\n",
    "        for chunk in df_reader:\n",
    "            process_json_chunk(chunk, csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd41963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16c4d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1195acff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68e9ee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_string_to_list(string):\n",
    "    \"\"\" \n",
    "    EXPLANATION:\n",
    "    This function will take a string and turn it into a stemmed list of words\n",
    "    \n",
    "    INPUT:\n",
    "        string:    string of words\n",
    "    \n",
    "    OUTPUT:\n",
    "        return:    list of words that are stemmed\n",
    "    \"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    return [ps.stem(word) for word in string.split()]\n",
    "\n",
    "def stem_list_to_list(arr):\n",
    "    \"\"\" \n",
    "    EXPLANATION:\n",
    "    This function will take a string and turn it into a stemmed list of words\n",
    "    \n",
    "    INPUT:\n",
    "        arr:    list of words that are stemmed\n",
    "    \n",
    "    OUTPUT:\n",
    "        return:    list of words that are stemmed\n",
    "    \"\"\"\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    return [ps.stem(word) for word in arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed71deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_list = [\"apple inc\", \"tim cook\", \"steve jobs\", \"iphone\", \"ipad\", \"imac\", \"apple watch\", \"macbook\", \"macbook pro\", \"aapl\", \"mac mini\"]\n",
    "check_list = [\"apple\"]\n",
    "exclude_list = [\"fruit\", 'banana', 'orange', 'strawberries', \"watermelon\", 'lemons']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2fa1f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_chunk(chunk, stem_add_list, stem_check_list, stem_exclude_list, filename):\n",
    "    \"\"\" \n",
    "    EXPLANATION:\n",
    "    This function will filter a chunk of data, based on the stemmed lists, \n",
    "    and write the result to the filename filepath\n",
    "    \n",
    "    INPUT:\n",
    "        chunk:              Chunk of data in a dataframe format\n",
    "        stem_add_list:      Stemmed list of sentences that will NOT be filtered out\n",
    "        stem_check_list:    Stemmed list of sentences that will MAYBE not be filtered out\n",
    "        stem_exlude_list:   Stemmed list of steneces that will be filtered out\n",
    "        filename:           Filepath of where the filtered data will be written to\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f'Processing chunk with {len(chunk)} rows')\n",
    "    csv_lines = []\n",
    "    \n",
    "    # Iterates over the chunk - row by row\n",
    "    for index, row in chunk.iterrows():\n",
    "        stemmed_quote_list = frozenset(stem_string_to_list(row['quotation']))\n",
    "\n",
    "        #========================================================\n",
    "        # Starting to check if quote contains any must have words\n",
    "        if not stemmed_quote_list.isdisjoint(stem_add_list):\n",
    "            csv_lines.append(row)\n",
    "\n",
    "        #========================================================\n",
    "        # Starting to check if quote contains any could have words\n",
    "        elif (not stemmed_quote_list.isdisjoint(stem_check_list)) and stemmed_quote_list.isdisjoint(stem_exclude_list):\n",
    "            csv_lines.append(row)\n",
    "            \n",
    "    \n",
    "    # Writing all the csv_lines that where not filtered out to the csv file\n",
    "    with open(filename, 'a', encoding='UTF8', newline='') as f: \n",
    "        writer = csv.writer(f, delimiter=';')\n",
    "        writer.writerows(csv_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3904963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out_apple_from_csv(csv_file_path, filtered_csv_file_path, add_list, check_list, exclude_list):\n",
    "    \"\"\" \n",
    "    EXPLANATION:\n",
    "    This function will filter out data from a csv_file_path, and write the filtered data to \n",
    "    a new filtered_csv_file_path.\n",
    "    \n",
    "    INPUT:\n",
    "        csv_file_path:             Filepath to where the UNFILTERED data is located\n",
    "        filtered_csv_file_path:    Filepath to where the FILTERED data will be written\n",
    "        stem_add_list:             Stemmed list of sentences that will NOT be filtered out\n",
    "        stem_check_list:           Stemmed list of sentences that will MAYBE not be filtered out\n",
    "        stem_exlude_list:          Stemmed list of steneces that will be filtered out\n",
    "        filename:                  Filepath of where the filtered data will be written to\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup variables\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    \n",
    "    # Checks if all the files are in order\n",
    "    if not os.path.isfile(filtered_csv_file_path):\n",
    "        # Creates a new file it it does not exists\n",
    "        f = open(filtered_csv_file_path, 'w')\n",
    "        writer = csv.writer(f, delimiter=';')\n",
    "        writer.writerow(['quoteID', 'quotation', 'speaker', 'date', 'numOccurrences']) \n",
    "        f.close()\n",
    "    if not os.path.isfile(csv_file_path):\n",
    "        raise Exception(\"You will need to input valid csv_file_path!\")\n",
    "    \n",
    "    # Stemming the lists we will use to filter quoteID;quotation;speaker;date;numOccurrences\n",
    "    stem_add_set = frozenset(stem_list_to_list(add_list))\n",
    "    stem_check_set = frozenset(stem_list_to_list(check_list))\n",
    "    stem_exclude_set = frozenset(stem_list_to_list(exclude_list))\n",
    "    \n",
    "    \n",
    "    # Start the iterative processing of the csv files\n",
    "    for chunk in pd.read_csv(csv_file_path, chunksize=200000, delimiter=';'):\n",
    "        process_csv_chunk(chunk, stem_add_list, stem_check_list, stem_exclude_list, filtered_csv_file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece9729f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec7b9b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk with 1000000 rows\n",
      "Processing chunk with 1000000 rows\n",
      "Processing chunk with 1000000 rows\n",
      "Processing chunk with 1000000 rows\n",
      "Processing chunk with 1000000 rows\n",
      "Processing chunk with 244449 rows\n"
     ]
    }
   ],
   "source": [
    "compressed_json_to_csv(\"data/quotes-2020.json.bz2\", \"data/quotes-2020-first-filter.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4852dcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk with 200000 rows\n",
      "Processing chunk with 200000 rows\n",
      "Processing chunk with 200000 rows\n",
      "Processing chunk with 200000 rows\n"
     ]
    }
   ],
   "source": [
    "filter_out_apple_from_csv(\"data/quotes-2020-first-filter.csv\", \n",
    "                          \"data/quotes-2020-apple-filter_2.csv\", \n",
    "                          add_list, check_list, exclude_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a1ad67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
